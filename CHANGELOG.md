# DataFlow Changelog

## [0.12.0] - 2026-02-21

### Quality Milestone Release - V4 Audit Cleared

This release completes 4 rounds of production quality audits (V1-V4) with all DataFlow-specific gaps remediated.

### Added

- **Auto-Wired Multi-Tenancy**: QueryInterceptor injects tenant filtering at 8 SQL execution points automatically
- **Async Transactions**: Transaction nodes are AsyncNode subclasses with proper `async_run()` pattern
- **Debug Persistence**: KnowledgeBase supports persistent SQLite storage for debug patterns
- **Savepoint Validation**: Regex-validated savepoint names prevent SQL injection in transaction nodes

### Changed

- **Bare Exception Cleanup**: All 4 bare `except:` blocks in engine.py replaced with `except Exception:`
- **SQL Injection Prevention**: Enhanced `_is_invalid_identifier()` with comprehensive SQL keyword blacklist
- **Sensitive Value Masking**: All logging paths use `mask_sensitive_values()` for credential safety

### Security

- Parameterized queries throughout (no f-string interpolation in SQL)
- Savepoint names validated via `^[A-Za-z_][A-Za-z0-9_]{0,62}$` regex
- Table/column/schema names validated before use in DDL
- Default values validated for injection patterns
- V4 audit: 0 CRITICAL, 0 HIGH findings

### Test Results

- 794 unit tests passed

## [0.10.12] - 2026-01-07

### Added

#### Centralized Logging Configuration (ADR-002)

- **New**: `LoggingConfig` dataclass for centralized log level control
  - `LoggingConfig.production()` - Only WARNING+ (production deployments)
  - `LoggingConfig.development()` - DEBUG level (local development)
  - `LoggingConfig.quiet()` - Only CRITICAL (testing with minimal output)
  - `LoggingConfig.from_env()` - Environment variable configuration
- **New**: `log_level` and `log_config` parameters in `DataFlow.__init__()`
  - `db = DataFlow("postgresql://...", log_level=logging.WARNING)`
  - `db = DataFlow("postgresql://...", log_config=LoggingConfig.production())`
- **New**: Category-specific log levels (node_execution, sql_generation, list_operations, migration, core)
- **New**: `mask_sensitive()` function for security-safe logging
- **New**: `configure_dataflow_logging()`, `restore_dataflow_logging()`, and `is_logging_configured()` utilities
- **New**: Environment variables for 12-factor app configuration:
  - `DATAFLOW_LOG_LEVEL` - Default level
  - `DATAFLOW_NODE_EXECUTION_LOG_LEVEL` - Node execution traces
  - `DATAFLOW_SQL_GENERATION_LOG_LEVEL` - SQL generation diagnostics
  - `DATAFLOW_MIGRATION_LOG_LEVEL` - Migration operations

### Fixed

#### Reduced WARNING Noise from 524 to 0 Messages

- **Fixed**: Node execution tracing messages incorrectly logged at WARNING level → DEBUG
- **Fixed**: SQL generation diagnostics incorrectly logged at WARNING level → DEBUG
- **Fixed**: ListNode field ordering info incorrectly logged at WARNING level → DEBUG
- **Fixed**: SQLite result tracing incorrectly logged at WARNING level → DEBUG
- **Fixed**: Core SDK node registration using root logger → named logger at INFO
- **Fixed**: Core SDK DDL safety check warnings during schema creation → DEBUG
- **Fixed**: Core SDK parameter validation warnings for expected behavior → DEBUG
- **Fixed**: Migration table creation attempted even when `migration_enabled=False`

### Changed

- Default logging behavior unchanged (WARNING level) for backward compatibility
- All diagnostic/trace messages now correctly logged at DEBUG level per ADR-002

---

## [0.10.2] - 2025-11-29

### Critical Bug Fixes

#### Session-Scoped Event Loop Deadlock Fixed (DATAFLOW-SESSION-LOOP-DEADLOCK-001)

- **Fixed**: `discover_schema()` causes deadlocks when called from pytest tests using session-scoped event loops (`asyncio_default_fixture_loop_scope = session`)
- **Bug ID**: DATAFLOW-SESSION-LOOP-DEADLOCK-001
- **Root Cause**: The v0.10.1 fix using `ThreadPoolExecutor + asyncio.run()` creates a NEW event loop in the worker thread, which cannot access connection pools tied to the ORIGINAL session-scoped pytest event loop, causing `future.result()` to block forever
- **Location**: `src/dataflow/core/engine.py:2545-2810, 5033-5095`
- **Solution**: Implemented async-first API pattern:
  1. `discover_schema()` now raises `RuntimeError` when called from a running async context with clear guidance
  2. Added `discover_schema_async()` for safe use in async contexts (uses existing event loop)
  3. Updated `_get_table_columns()` to handle async context gracefully with fallback
  4. Added `_get_table_columns_async()` for async contexts
- **Impact**:
  - ✅ Clear error message in async contexts prevents silent deadlocks
  - ✅ `discover_schema_async()` works correctly with session-scoped pytest event loops
  - ✅ `_get_table_columns()` no longer triggers deadlock in async workflows
  - ✅ Backward compatible: sync code paths unchanged
- **Breaking**: NO - adds new methods, existing sync usage unchanged
- **Example**:

  ```python
  # Before: Hangs indefinitely in session-scoped pytest fixtures
  # ThreadPoolExecutor creates new loop that can't access session-scoped pool

  # After: Clear error with guidance
  async def test_with_session_loop(dataflow_instance):
      try:
          schema = dataflow_instance.discover_schema(use_real_inspection=True)
      except RuntimeError as e:
          # "discover_schema() cannot be called from a running async context.
          #  Use 'await discover_schema_async()' instead"
          pass

  # Solution: Use async version
  async def test_with_session_loop(dataflow_instance):
      schema = await dataflow_instance.discover_schema_async(use_real_inspection=True)
      # Works correctly with session-scoped event loop!
  ```

- **New Methods**:
  - `discover_schema_async(use_real_inspection: bool = False)` - Async version for async contexts
  - `_get_table_columns_async(table_name: str)` - Async version for internal use
- **Test Coverage**: 3 comprehensive tests
  - Sync context test (existing behavior preserved)
  - Async context RuntimeError test (new protection)
  - Async version test (new functionality)
- **Test Results**: 3/3 passing (100% success rate)
- **Files Modified**:
  - `src/dataflow/core/engine.py` (discover_schema, discover_schema_async, \_get_table_columns, \_get_table_columns_async, \_generate_mock_schema_data)
  - `test_session_scoped_loop_deadlock.py` (reproduction test)

---

## [0.10.1] - 2025-11-28

### Critical Bug Fixes

#### Nested Event Loop Deadlock Fixed (DATAFLOW-NESTED-LOOP-001)

- **Fixed**: `discover_schema()` hangs/deadlocks when called from async context
- **Bug ID**: DATAFLOW-NESTED-LOOP-001
- **Root Cause**: `discover_schema()` called `asyncio.run()` while already inside an async event loop, causing deadlock
- **Location**: `src/dataflow/core/engine.py:2559`
- **Solution**: Use `ThreadPoolExecutor` to run async operation in separate thread when already in async context
- **Impact**:
  - ✅ `discover_schema()` can now be safely called from async functions
  - ✅ No deadlock or hanging when used in async contexts (FastAPI, async workflows, etc.)
  - ✅ Works correctly with or without `nest_asyncio` installed
  - ✅ Maintains backward compatibility with sync code paths
- **Breaking**: NO - fully backward compatible, transparent to users
- **Note**: See v0.10.2 for follow-up fix addressing session-scoped pytest event loops

---

## [0.9.7] - 2025-11-25

### Critical Bug Fixes

#### Nested Event Loop Deadlock Fixed (DATAFLOW-NESTED-LOOP-001)

- **Fixed**: `discover_schema()` hangs/deadlocks when called from async context
- **Bug ID**: DATAFLOW-NESTED-LOOP-001
- **Root Cause**: `discover_schema()` called `asyncio.run()` while already inside an async event loop, causing deadlock
- **Location**: `src/dataflow/core/engine.py:2559`
- **Solution**: Use `ThreadPoolExecutor` to run async operation in separate thread when already in async context
  - Removed dependency on `nest_asyncio` (which masked but didn't fix the underlying issue)
  - Always use `ThreadPoolExecutor` when existing event loop is detected
  - Continue using `asyncio.run()` when no event loop is running (safe case)
- **Impact**:
  - ✅ `discover_schema()` can now be safely called from async functions
  - ✅ No deadlock or hanging when used in async contexts (FastAPI, async workflows, etc.)
  - ✅ Works correctly with or without `nest_asyncio` installed
  - ✅ Maintains backward compatibility with sync code paths
- **Breaking**: NO - fully backward compatible, transparent to users
- **Example**:

  ```python
  # Before: Hangs indefinitely when called from async context
  # asyncio.run() cannot be called from a running event loop

  # After: Works correctly in all contexts
  import asyncio
  from dataflow import DataFlow

  async def async_function():
      db = DataFlow("postgresql://...")
      schema = db.discover_schema(use_real_inspection=True)  # No deadlock!
      return schema

  # Also works in sync contexts (no change)
  db = DataFlow("postgresql://...")
  schema = db.discover_schema(use_real_inspection=True)  # Still works
  ```

- **Test Coverage**: 2 comprehensive tests
  - Direct async context test (most common failure scenario)
  - run_in_executor wrapper test (FastAPI/async web framework scenario)
- **Test Results**: 2/2 passing (100% success rate)
- **Files Modified**:
  - `src/dataflow/core/engine.py` (discover_schema method - simplified to always use ThreadPoolExecutor in async contexts)

#### Cache Async/Await Bug Fixed (DATAFLOW-CACHE-ASYNC-001)

- **Fixed**: `TypeError: 'coroutine' object does not support item assignment` in cache operations when using `InMemoryCache`
- **Bug ID**: DATAFLOW-CACHE-ASYNC-001
- **Root Cause**: `ListNodeCacheIntegration` called async cache methods without `await`, treating coroutines as regular values
- **Location**: `src/dataflow/cache/list_node_integration.py:74, 88, 108`
- **Solution**: Implemented unified async cache interface across all backends
  - Created `AsyncRedisCacheAdapter` to wrap sync `RedisCacheManager` with async interface
  - Added `await` to 3 cache method calls in `ListNodeCacheIntegration`
  - Normalized `get_metrics()` response format across InMemoryCache and Redis backends
  - Added `get_metrics()` and `invalidate_model()` methods to `AsyncRedisCacheAdapter`
- **Impact**:
  - ✅ InMemoryCache (native async) works correctly with await
  - ✅ RedisCacheManager (sync) wrapped with `AsyncRedisCacheAdapter` for async compatibility
  - ✅ ListNode cache operations no longer throw TypeError
  - ✅ Unified async interface across all cache backends
- **Breaking**: NO - fully backward compatible, transparent to users
- **Example**:

  ```python
  # Before: Failed with TypeError
  # TypeError: 'coroutine' object does not support item assignment

  # After: Works correctly with both cache backends
  from dataflow.cache.auto_detection import CacheBackend
  from dataflow.cache.list_node_integration import ListNodeCacheIntegration

  # Auto-detect returns either InMemoryCache or AsyncRedisCacheAdapter
  cache = CacheBackend.auto_detect()  # Both have unified async interface

  # Use with ListNode operations (now with proper await)
  result = await integration.execute_with_cache(
      model_name="User",
      query="SELECT * FROM users",
      params=[],
      executor_func=lambda: {"data": "value"},
      cache_enabled=True
  )
  ```

- **Test Coverage**: 53 comprehensive tests across 3 tiers
  - Tier 1 (Unit): 32 tests for AsyncRedisCacheAdapter (all passing)
  - Tier 2 (Integration): 15 tests with real InMemoryCache (all passing)
  - Tier 3 (E2E): 6 tests with complete workflows (all passing)
- **Test Results**: 53/53 passing (100% success rate)
- **Files Modified**:
  - `src/dataflow/cache/async_redis_adapter.py` (NEW - 370 lines)
  - `src/dataflow/cache/__init__.py` (added AsyncRedisCacheAdapter export)
  - `src/dataflow/cache/auto_detection.py` (returns AsyncRedisCacheAdapter for Redis)
  - `src/dataflow/cache/list_node_integration.py` (added await to 3 cache calls)

#### Model Registration Race Condition Fixed

- **Fixed**: Race condition in pytest where models imported during test collection phase failed to register because `dataflow_model_registry` table didn't exist yet
- **Root Cause**: Model `@db.model` decorators executed at import time (before table creation), triggering immediate registration queries that failed in pytest collection phase
- **Location**: `src/dataflow/core/model_registry.py:92-167, 311-335`
- **Solution**: Implemented lazy model registration queue system
  - Models queue for registration before initialization (`_pending_models`)
  - Registry initialization automatically processes pending models (`_finalize_initialization()`)
  - Thread-safe with `threading.Lock()` protection for concurrent registration
  - Backward compatible: Initialized registries register models immediately (no queue)
- **Impact**:
  - ✅ Pytest tests now work correctly (all 32 Kaizen Studio models register successfully)
  - ✅ Standalone scripts unchanged (registry auto-initializes during DataFlow construction)
  - ✅ Production deployments protected from import-time registration failures
- **Breaking**: NO - fully backward compatible, transparent to users
- **Example**:

  ```python
  # Before: Failed in pytest collection phase
  # ERROR: relation "dataflow_model_registry" does not exist

  # After: Models queue during import, register after initialization
  db = DataFlow("postgresql://...")  # Registry initializes

  @db.model  # Registers immediately (or queues if not initialized)
  class User:
      id: str
      name: str

  # Pytest collection phase: Models queue successfully
  # Test execution phase: Models registered when test_db fixture runs
  ```

- **Test Coverage**: 15 comprehensive unit tests covering:
  - Model queueing before initialization
  - Immediate registration after initialization
  - Finalization process
  - Thread safety
  - Error handling
  - Backward compatibility
- **Test Results**: 15/15 passing (`tests/unit/test_lazy_model_registration.py`)

#### Database Infrastructure Threading Issue Fixed

- **Fixed**: `'tuple' object has no attribute 'execute'` errors when using `AsyncSQLDatabaseNode` with synchronous `LocalRuntime` for DDL operations
- **Root Cause**: Model registry and schema state manager used asynchronous `AsyncSQLDatabaseNode` with synchronous `LocalRuntime`, which returned tuples instead of database results
- **Location**:
  - `src/dataflow/core/model_registry.py` (17 instances)
  - `src/dataflow/migrations/schema_state_manager.py` (4 instances)
- **Solution**: Replaced all `AsyncSQLDatabaseNode` with synchronous `SQLDatabaseNode` for DDL operations
  - DDL operations (CREATE TABLE, CREATE INDEX) now use `SQLDatabaseNode` with `LocalRuntime`
  - Parameter naming corrected from `"params"` to `"parameters"` (8 instances)
  - Works correctly in all contexts: sync, async, and pytest
- **Impact**:
  - ✅ All DataFlow DDL operations work correctly (table creation, index creation)
  - ✅ No more runtime/node type mismatches
  - ✅ Compatible with all runtime contexts
- **Breaking**: NO - internal implementation change only
- **Performance**: No impact (DDL operations are infrequent)
- **Test Coverage**: All 46 DataFlow core tests passing

### Documentation Updates

#### Test Expectations Updated

- **Updated**: Integration test expectations to reflect lazy registration behavior
- **Location**: `tests/unit/test_lazy_model_registration.py:340-367`
- **Changes**:
  - Registry now auto-initializes during DataFlow construction (correct behavior)
  - Models register immediately instead of queueing (registry already initialized)
  - Updated assertions to expect `_initialized=True` and `_pending_models=0`
- **Rationale**: Tests now verify correct behavior (auto-initialization) instead of incorrect expectations

### Migration Guide

#### No Action Required for Users

This release is **100% backward compatible**:

- ✅ Existing code works unchanged
- ✅ No API changes
- ✅ No configuration changes
- ✅ No breaking changes

#### Benefits for Users

1. **Pytest Compatibility**: Tests using DataFlow models now work correctly
2. **Production Safety**: Import-time registration failures prevented
3. **Better Error Handling**: Graceful fallback if initialization fails

#### Internal Changes Only

- Model registration uses queue system (transparent to users)
- DDL operations use synchronous SQLDatabaseNode (internal implementation)
- No user-facing API changes

### Verification

#### Test Results

- **Lazy Registration Tests**: 15/15 passing ✅
- **DataFlow Core Tests**: 46/46 passing ✅
- **No Regressions**: All existing functionality preserved ✅

#### Verified Scenarios

1. ✅ Standalone scripts (registry auto-initializes)
2. ✅ Pytest tests (models queue during collection, register during execution)
3. ✅ Multi-threaded applications (thread-safe registration)
4. ✅ FastAPI/Gunicorn deployments (protected from import-time failures)

---

## [0.7.12] - 2025-11-02

### Bug Fixes

#### Bulk Operations Rowcount Extraction Fixed

- **Fixed**: `bulk_create` incorrectly prioritized `row_count` field over `data.rows_affected`, causing inaccurate reporting
- **Location**: `src/dataflow/features/bulk.py:342-368`
- **Root Cause**: Extraction logic checked `row_count` first (calculated from `len(data)` = 1), instead of `data[0]['rows_affected']` (actual database rowcount)
- **Solution**: Reversed extraction priority to check `data` field FIRST, then fall back to `row_count` for backward compatibility
- **Example**:
  ```python
  # Before: Reported 1 record created when 3 were actually created
  # After: Correctly reports 3 records created
  workflow.add_node("ProductBulkCreateNode", "import", {
      "data": [{"id": "1"}, {"id": "2"}, {"id": "3"}]
  })
  ```
- **Impact**: All bulk operations now accurately report database operation counts
- **Related**: Requires Core SDK v0.10.6+ for proper rowcount capture from database adapters
- **Breaking**: NO - fully backward compatible, fixes reporting accuracy only

---

## [0.7.11] - 2025-10-31

### Bug Fixes

#### Bulk Operations Parameter Handling Fixed

- **Fixed**: `TypeError: got multiple values for keyword argument 'model_name'` in all 4 bulk operations (BulkCreate, BulkUpdate, BulkDelete, BulkUpsert)
- **Location**: `src/dataflow/core/nodes.py` lines 2835, 2951-2952, 3054-3055, 3116
- **Root Cause**: Bulk operations passed explicit parameters (`model_name`, `db_instance`) and then spread `**kwargs` without filtering those same parameters, causing conflicts when global workflow inputs contained these parameters
- **Solution**: Added `"model_name"` and `"db_instance"` to exclusion lists in all 4 bulk operations' kwargs filtering
- **Impact**: All bulk operations now work correctly with Nexus/AsyncLocalRuntime global parameters
- **Breaking**: NO - fully backward compatible, no API changes
- **Example**:
  ```python
  # Now works correctly with global parameters:
  workflow.add_node("ProductBulkDeleteNode", "cleanup", {
      "filter": {"active": False}
  })
  results, _ = await runtime.execute_workflow_async(
      workflow.build(),
      inputs={"model_name": "Product", "user_id": "admin"}  # Global params no longer cause conflicts
  )
  ```

---

## [0.7.10] - 2025-10-30

### New Features

#### Test Mode API (ADR-017)

- **Added**: Comprehensive Test Mode API for production-grade async testing
- **Features**:
  - 3-tier auto-detection (explicit parameter > global setting > auto-detection)
  - Global test mode control via `DataFlow.enable_test_mode()`, `disable_test_mode()`, `is_test_mode_enabled()`
  - Connection pool cleanup methods: `cleanup_stale_pools()`, `cleanup_all_pools()`, `get_cleanup_metrics()`
  - Thread-safe with RLock protection for multi-threaded applications
  - Zero overhead (<150ms per test with aggressive cleanup)
- **Benefits**:
  - Eliminates "Event loop is closed" errors in pytest
  - Prevents pool leaks between tests
  - Automatic detection when running under pytest
  - Graceful error handling with detailed metrics
- **Location**: `src/dataflow/core/engine.py:270-1600`
- **Breaking**: NO - fully backward compatible, opt-in feature
- **Documentation**: See `/apps/kailash-dataflow/adr/ADR-017-*.md` (6 files) for complete specification

#### AsyncSQLDatabaseNode Enhancements

- **Added**: Async-first cleanup method `_cleanup_closed_loop_pools()` (async class method)
- **Enhanced**: `clear_shared_pools()` now accepts `graceful` parameter with detailed metrics return
- **Added**: `_total_pools_created` counter for lifecycle tracking
- **Benefits**:
  - Proper async handling (no more "object int can't be used in await" errors)
  - Graceful pool cleanup with error reporting
  - Complete pool lifecycle visibility
- **Location**: `src/kailash/nodes/data/async_sql.py:2371-3500`
- **Breaking**: NO - backward compatible, enhanced API

### Test Coverage

- **Added**: 33 comprehensive unit tests covering all Test Mode API features
- **Coverage**: Test mode detection (7 tests), global control (4 tests), priority system (5 tests), logging (4 tests), cleanup methods (8 tests), graceful degradation (3 tests), backward compatibility (3 tests)
- **Result**: 100% passing (33/33 tests)
- **Location**: `tests/unit/core/test_dataflow_test_mode.py`

### Documentation Updates

- **Added**: Complete Test Mode API documentation in dataflow-specialist subagent
- **Sections**: API overview, configuration, cleanup methods, fixture patterns, troubleshooting
- **Location**: `.claude/agents/frameworks/dataflow-specialist.md:923-1103`
- **Quick Reference**: Test Mode Configuration table added to Quick Config section

## [0.6.3] - 2025-10-22

### Bug Fixes

#### BulkDeleteNode Safe Mode Validation Fixed

- **Fixed**: Similar truthiness bug in BulkDeleteNode safe mode validation
- **Location**: `src/dataflow/nodes/bulk_delete.py:177`
- **Root Cause**: `not filter_conditions` evaluates to True for empty dict `{}`, incorrectly rejecting valid operations
- **Solution**: Changed from `not filter_conditions` to `"filter" not in validated_inputs` to match pattern at line 153
- **Impact**: BulkDeleteNode safe_mode now correctly handles empty filter operations
- **Discovery**: Found during comprehensive search for similar bugs after v0.6.2 fix
- **Consistency**: Makes line 177 consistent with line 153's validation logic
- **Documentation**: See SIMILAR_BUGS_SEARCH_REPORT.md for complete search results
- **Breaking**: NO - backward compatible, fixes edge case

### Comprehensive Bug Search

- **Searched**: 50+ files, 100+ code locations, 13 suspicious patterns found
- **Result**: 1 real bug found and fixed (bulk_delete.py), 12 false positives (correct behavior)
- **Confidence**: Very High (95%+) - All similar truthiness bugs have been found and fixed
- **Report**: SIMILAR_BUGS_SEARCH_REPORT.md contains full methodology and findings

## [0.6.2] - 2025-10-22

### Critical Bug Fixes

#### ListNode Filter Operators Fixed

- **Fixed**: Critical bug where all MongoDB-style filter operators ($ne, $nin, $in, $not) were broken in ListNode except $eq
- **Root Cause**: Python truthiness bug - `if filter_dict:` evaluates to False for empty dict `{}`, causing QueryBuilder path to be skipped
- **Solution**: Changed condition from `if filter_dict:` to `if "filter" in kwargs:` at line 1810 in nodes.py
- **Impact**: All filter operators now work correctly - $ne, $nin, $in, $not, $gt, $lt, $gte, $lte, $regex, etc.
- **Evidence**: SQL query logging confirms QueryBuilder path is now used correctly with proper WHERE clauses
- **Example**:
  ```python
  # Now works correctly:
  workflow.add_node("UserListNode", "list_active", {
      "filter": {"status": {"$ne": "inactive"}}
  })
  # Generates: SELECT * FROM "users" WHERE "status" != $1
  ```
- **Files Changed**: `src/dataflow/core/nodes.py:1810`
- **Documentation**: See BUGFIX_EVIDENCE.md for complete proof
- **Matches**: v0.5.2 fix pattern for BulkUpdateNode and BulkDeleteNode
- **Breaking**: NO - backward compatible, fixes broken functionality

## [0.6.1] - 2025-10-22

### Documentation

#### Comprehensive Documentation Updates

- **Updated**: Complete documentation refresh for DataFlow framework
- **New Guides**: Added specialized guides for bulk operations, migrations, multi-tenancy, and performance
- **Updated Version**: All documentation now references DataFlow 0.6.0+
- **Files Updated**:
  - `.claude/skills/02-dataflow/*.md` - 13 comprehensive guides
  - Examples, patterns, and best practices updated
- **Breaking**: NO - documentation only

## [0.6.0] - 2025-10-21

### Major Features

#### MongoDB Document Database Support

- **Added**: Complete MongoDB document database support via MongoDBAdapter
- **Impact**: Enables NoSQL applications, flexible schema operations, and rapid iteration with document-based data models
- **Components**:
  - `MongoDBAdapter` - Extends BaseAdapter with Motor async driver for MongoDB operations
  - `DocumentInsertNode` - Insert single document workflow node
  - `DocumentFindNode` - Find documents with filters, sorting, and pagination
  - `DocumentUpdateNode` - Update one or many documents
  - `DocumentDeleteNode` - Delete one or many documents
  - `AggregateNode` - Execute MongoDB aggregation pipelines
  - `BulkDocumentInsertNode` - Bulk insert documents
  - `CreateIndexNode` - Create indexes (simple or compound)
  - `DocumentCountNode` - Count documents matching filter
- **Features**:
  - Flexible schema (schemaless) document operations
  - MongoDB Query Language support (comparison, logical, array operators)
  - Aggregation pipelines for complex data processing
  - Index management (single, compound, text, geospatial)
  - Collection management (create, drop, list, exists)
  - Connection pooling with Motor async driver
  - Health checks and comprehensive error handling
- **Files**:
  - `src/dataflow/adapters/mongodb.py` (870 lines) - MongoDB adapter implementation
  - `src/dataflow/nodes/mongodb_nodes.py` (910 lines) - 8 MongoDB workflow nodes
- **Tests**: 83 comprehensive tests (100% passing)
  - `tests/unit/adapters/test_mongodb_adapter.py` (850+ lines) - 43 adapter tests
  - `tests/unit/nodes/test_mongodb_nodes.py` (900+ lines) - 40 node tests
  - All tests passed in 0.61s
- **Documentation**:
  - `docs/guides/mongodb-quickstart.md` (800+ lines) - Complete user guide
  - `docs/architecture/mongodb-implementation-plan.md` - Architecture specification
  - `examples/mongodb_crud_example.py` (400+ lines) - Complete CRUD example
- **Dependencies**:
  - `motor>=3.3.0` - MongoDB async driver
  - `pymongo>=4.5.0` - Motor dependency
  - `dnspython>=2.4.0` - For mongodb+srv:// URLs
- **Breaking**: NO - fully backward compatible, opt-in feature

**Example Usage**:

```python
from dataflow import DataFlow
from dataflow.adapters import MongoDBAdapter

# Create MongoDB adapter
adapter = MongoDBAdapter(
    "mongodb://localhost:27017/mydb",
    maxPoolSize=50,
    minPoolSize=10
)
db = DataFlow(adapter=adapter)
await db.initialize()

# Document operations using adapter
user_id = await adapter.insert_one("users", {
    "name": "Alice",
    "email": "alice@example.com",
    "age": 30,
    "tags": ["developer", "python"]
})

users = await adapter.find("users",
    filter={"age": {"$gte": 25}},
    sort=[("name", 1)],
    limit=10
)

# Workflow integration
from dataflow.nodes.mongodb_nodes import DocumentFindNode, AggregateNode
from kailash.workflow.builder import WorkflowBuilder

workflow = WorkflowBuilder()

# Find active users
workflow.add_node("DocumentFindNode", "find_users", {
    "collection": "users",
    "filter": {"status": "active"},
    "sort": [("name", 1)]
})

# Aggregate sales by category
workflow.add_node("AggregateNode", "sales_summary", {
    "collection": "orders",
    "pipeline": [
        {"$match": {"status": "completed"}},
        {"$group": {
            "_id": "$category",
            "total": {"$sum": "$amount"}
        }},
        {"$sort": {"total": -1}}
    ]
})

results = await runtime.execute_workflow_async(workflow.build())
```

**MongoDB vs SQL Comparison**:

```python
# SQL Approach (PostgreSQL)
db = DataFlow("postgresql://localhost/mydb")

@db.model
class User:
    id: int
    name: str
    email: str

# MongoDB Approach (Flexible Schema)
adapter = MongoDBAdapter("mongodb://localhost:27017/mydb")
db = DataFlow(adapter=adapter)

# No model definition needed - direct document operations
await adapter.insert_one("users", {
    "name": "Alice",
    "email": "alice@example.com",
    "profile": {"age": 30, "city": "NYC"},  # Nested documents
    "tags": ["developer", "python"],         # Arrays
    # Any fields, no schema constraints!
})
```

### Multi-Database Support Matrix

DataFlow now supports 4 database types:

| Database                  | Adapter                   | Use Case                          | Schema         | Query Language         |
| ------------------------- | ------------------------- | --------------------------------- | -------------- | ---------------------- |
| **PostgreSQL**            | `PostgreSQLAdapter`       | Production, complex queries, ACID | Fixed          | SQL                    |
| **PostgreSQL + pgvector** | `PostgreSQLVectorAdapter` | RAG, semantic search              | Fixed + Vector | SQL + Vector           |
| **MongoDB**               | `MongoDBAdapter`          | Flexible schema, rapid iteration  | Schemaless     | MongoDB Query Language |
| **SQLite**                | `SQLiteAdapter`           | Development, embedded, mobile     | Fixed          | SQL                    |

### Documentation Updates

- Added MongoDB roadmap to `.claude/skills/02-dataflow/SKILL.md`
- Complete MongoDB quickstart guide with CRUD examples
- Architecture decision records for MongoDB implementation
- README updated with MongoDB support information

### Testing Coverage

- **NO MOCKING** policy maintained for integration tests
- 83 unit tests for MongoDB adapter and nodes (100% passing)
- Comprehensive test coverage for document operations, queries, aggregation, indexing

#### PostgreSQL Vector Similarity Search (pgvector Support)

- **Added**: Complete vector similarity search support via PostgreSQLVectorAdapter
- **Impact**: Enables RAG applications, semantic search, and hybrid search with 40-60% cost savings vs dedicated vector databases
- **Components**:
  - `PostgreSQLVectorAdapter` - Extends PostgreSQLAdapter with vector operations
  - `VectorSearchNode` - Semantic similarity search workflow node
  - `CreateVectorIndexNode` - Vector index creation workflow node
  - `HybridSearchNode` - Combined vector + full-text search workflow node
- **Features**:
  - Multiple distance metrics: cosine, L2, inner product
  - IVFFlat and HNSW index types
  - Hybrid search with RRF (Reciprocal Rank Fusion)
  - Filter-based vector search
  - Vector column statistics
- **Files**:
  - `src/dataflow/adapters/postgresql_vector.py` (465 lines) - Vector adapter implementation
  - `src/dataflow/nodes/vector_nodes.py` (460 lines) - Vector workflow nodes
- **Tests**: 40 comprehensive tests (24 unit, 16 integration)
  - `tests/unit/adapters/test_postgresql_vector_adapter.py` (443 lines)
  - `tests/unit/nodes/test_vector_nodes.py` (566 lines)
  - `tests/integration/adapters/test_postgresql_vector_adapter_integration.py` (340 lines)
  - `tests/integration/nodes/test_vector_nodes_integration.py` (290 lines)
- **Documentation**:
  - `docs/guides/pgvector-quickstart.md` - Complete user guide
  - `docs/architecture/pgvector-implementation-plan.md` - Architecture specification
  - `docs/pgvector-implementation-summary.md` - Implementation summary
- **Breaking**: NO - fully backward compatible, opt-in feature

**Example Usage**:

```python
from dataflow import DataFlow
from dataflow.adapters import PostgreSQLVectorAdapter

# Create vector adapter
adapter = PostgreSQLVectorAdapter(
    "postgresql://localhost/vectordb",
    vector_dimensions=1536,  # OpenAI embeddings
    default_distance="cosine"
)
db = DataFlow(adapter=adapter)

# Semantic search
from dataflow.nodes.vector_nodes import VectorSearchNode
from kailash.workflow.builder import WorkflowBuilder

workflow = WorkflowBuilder()
workflow.add_node("VectorSearchNode", "search", {
    "table_name": "documents",
    "query_vector": embedding,  # 1536-dim vector from AI model
    "k": 5,
    "distance": "cosine"
})

results = await runtime.execute_workflow_async(workflow.build())
documents = results["search"]["results"]  # Top 5 similar documents
```

#### BaseAdapter Hierarchy

- **Added**: Minimal base interface for all adapter types (SQL, Document, Vector, Graph, Key-Value)
- **Impact**: Foundation for multi-database support beyond SQL
- **Component**: `src/dataflow/adapters/base_adapter.py` (133 lines)
- **Changes**: DatabaseAdapter now inherits from BaseAdapter
- **Tests**: 10 comprehensive tests in `tests/unit/adapters/test_base_adapter_hierarchy.py`
- **Breaking**: NO - fully backward compatible

**Adapter Hierarchy**:

```
BaseAdapter (minimal interface)
├── DatabaseAdapter (SQL databases)
│   ├── PostgreSQLAdapter
│   │   └── PostgreSQLVectorAdapter (vector search)
│   ├── MySQLAdapter
│   └── SQLiteAdapter
└── Future: DocumentAdapter, VectorAdapter, GraphAdapter, KeyValueAdapter
```

### Performance

#### Vector Search Benchmarks

- **Query Latency**: <50ms for 100K vectors (with IVFFlat index)
- **Index Build**: <5 minutes for 1M vectors (IVFFlat)
- **Memory**: <2GB for 1M vectors (1536 dimensions)
- **Throughput**: >100 QPS for semantic search operations

### Documentation Updates

- Added pgvector roadmap to `.claude/skills/02-dataflow/SKILL.md`
- Added "Coming Soon" sections to README and CLAUDE.md
- Complete pgvector quickstart guide with RAG examples
- Architecture decision records for BaseAdapter hierarchy

### Testing Improvements

- **NO MOCKING** policy enforced for integration tests
- Real PostgreSQL + pgvector infrastructure testing
- Concurrent vector search tests
- Hybrid search integration tests

### Compatibility

- ✅ 100% backward compatible
- ✅ All existing tests passing (60+ tests)
- ✅ Zero breaking changes
- ✅ Opt-in feature (requires pgvector extension)

## [0.5.4] - 2025-10-11

### Critical Bug Fixes

#### Cache Invalidation Missing in Bulk Operations (HIGH PRIORITY)

- **Fixed**: BulkUpdateNode, BulkDeleteNode, and BulkUpsertNode now properly invalidate query cache after data modifications
- **Impact**: ListNode queries now return fresh database data instead of stale cached results
- **Root Cause**: Missing `cache_integration.invalidate_model_cache()` calls after successful bulk operations
- **Symptoms**: Applications using bulk operations with query caching were getting stale data, causing data consistency issues
- **Locations**:
  - `src/dataflow/core/nodes.py:1884-1897` - Added cache invalidation to BulkUpdateNode
  - `src/dataflow/core/nodes.py:1940-1953` - Added cache invalidation to BulkDeleteNode
  - `src/dataflow/core/nodes.py:1997-2010` - Added cache invalidation to BulkUpsertNode
- **Tests**: 3 comprehensive integration tests in `tests/integration/test_cache_invalidation_bug.py`
- **Breaking**: NO - previously broken functionality now works correctly

**Before**:

```python
# Step 1: Bulk delete all records
workflow.add_node('AgentMemoryBulkDeleteNode', 'cleanup', {
    'filter': {}, 'confirmed': True
})
runtime.execute(workflow.build())  # ❌ Cache NOT invalidated

# Step 2: Query after deletion
workflow.add_node('AgentMemoryListNode', 'query', {
    'filter': {'workflow_run_id': 300}
})
result, _ = runtime.execute(workflow.build())
# BUG: Returns old cached data instead of empty result
# {'records': [old_data], '_cache': {'hit': True}}
```

**After**:

```python
# Step 1: Bulk delete all records
workflow.add_node('AgentMemoryBulkDeleteNode', 'cleanup', {
    'filter': {}, 'confirmed': True
})
runtime.execute(workflow.build())  # ✅ Cache properly invalidated

# Step 2: Query after deletion
workflow.add_node('AgentMemoryListNode', 'query', {
    'filter': {'workflow_run_id': 300}
})
result, _ = runtime.execute(workflow.build())
# ✅ Returns fresh data from database: {'records': [], 'count': 0}
```

#### Async/Await Bug in BulkUpsertNode (CRITICAL)

- **Fixed**: BulkUpsertNode now properly awaits async `bulk_upsert()` function call
- **Impact**: Prevents runtime errors when bulk_upsert operations are executed
- **Root Cause**: Missing `await` keyword when calling async function
- **Location**: `src/dataflow/core/nodes.py:1982` - Added `await` keyword
- **Related**: `src/dataflow/features/bulk.py:564` - Changed `bulk_upsert` from `def` to `async def`
- **Breaking**: NO - fixes previously broken async execution

**Before**:

```python
# Missing await caused runtime errors
bulk_result = self.dataflow_instance.bulk.bulk_upsert(...)  # ❌ WRONG
```

**After**:

```python
# Properly awaits async function
bulk_result = await self.dataflow_instance.bulk.bulk_upsert(...)  # ✅ CORRECT
```

#### Return Structure Inconsistencies (HIGH PRIORITY)

- **Fixed**: BulkDeleteNode and BulkUpsertNode exception handlers now include operation-specific aliases
- **Impact**: API consistency across all bulk operations; better error handling
- **Root Cause**: Missing "deleted" and "upserted" aliases in exception return structures
- **Locations**:
  - `src/dataflow/core/nodes.py:1974` - Added "deleted": 0 to BulkDeleteNode exception handler
  - `src/dataflow/core/nodes.py:2041` - Added "upserted": 0 to BulkUpsertNode exception handler
- **Breaking**: NO - adds missing fields, maintains backward compatibility

**Before**:

```python
# BulkDeleteNode exception: missing "deleted" alias
return {
    "processed": 0,
    # "deleted": 0 - MISSING!
    "success": False,
    "error": str(e),
}
```

**After**:

```python
# BulkDeleteNode exception: includes "deleted" alias for API consistency
return {
    "processed": 0,
    "deleted": 0,  # Alias for compatibility
    "success": False,
    "error": str(e),
}
```

#### Error Propagation Gap in BulkUpsertNode (HIGH PRIORITY)

- **Fixed**: BulkUpsertNode now properly propagates error details and operational statistics
- **Impact**: Better debugging experience; detailed upsert statistics (inserted/updated/skipped)
- **Root Cause**: Missing error propagation and detailed stat exposure
- **Location**: `src/dataflow/core/nodes.py:2013-2036` - Enhanced return structure with error propagation
- **Breaking**: NO - adds additional information without breaking existing behavior

**Enhanced Return Structure**:

```python
result = {
    "processed": bulk_result.get("records_processed", 0),
    "upserted": bulk_result.get("records_processed", 0),  # Alias for compatibility
    "batch_size": batch_size,
    "operation": operation,
    "success": bulk_result.get("success", True),
}

# Expose detailed upsert stats if available
if "inserted" in bulk_result:
    result["inserted"] = bulk_result["inserted"]
if "updated" in bulk_result:
    result["updated"] = bulk_result["updated"]
if "skipped" in bulk_result:
    result["skipped"] = bulk_result["skipped"]

# Propagate error details if operation failed
if not bulk_result.get("success", True) and "error" in bulk_result:
    result["error"] = bulk_result["error"]
```

#### Mock Implementation Warning for bulk_upsert (CRITICAL)

- **Added**: Comprehensive warnings that bulk_upsert is currently a stub implementation
- **Impact**: Users are clearly informed that data is NOT being upserted to the database
- **Root Cause**: bulk_upsert returns simulated data without performing real database operations
- **Location**: `src/dataflow/features/bulk.py:564-607` - Added docstring warning and runtime logging
- **Breaking**: NO - exposes existing limitation with clear communication

**Warning Added**:

```python
async def bulk_upsert(...) -> Dict[str, Any]:
    """Perform bulk upsert (insert or update) operation.

    WARNING: This is currently a STUB implementation that returns simulated data.
    Real database upsert operations are NOT yet implemented.
    Data is NOT being inserted or updated in the database.
    """
    logger.warning(
        f"BULK_UPSERT WARNING: This is a STUB implementation! "
        f"No actual database operations are performed. "
        f"Data will NOT be inserted or updated. "
        f"Model: {model_name}, Records: {len(data)}"
    )
```

### Test Coverage

- **Cache Invalidation Tests**: 3/3 PASSED in `tests/integration/test_cache_invalidation_bug.py`
  - `test_bulk_delete_cache_invalidation` - Delete → List returns fresh empty result
  - `test_bulk_update_cache_invalidation` - Update → List returns fresh updated data
  - `test_bulk_create_then_delete_then_create_cache_bug` - Exact user scenario reproduction
- **Unit Tests**: 36/36 PASSED (100%)
- **NO REGRESSIONS**: All existing tests continue to pass
- **Infrastructure**: Real PostgreSQL testing (NO MOCKING policy)

### Files Changed

- `src/dataflow/core/nodes.py` - 7 separate fixes across bulk operations
- `src/dataflow/features/bulk.py` - Made bulk_upsert async with warnings
- `tests/integration/test_cache_invalidation_bug.py` - New comprehensive test suite
- `docs/bugfix-v054-cache-invalidation.md` - Complete technical documentation

### Impact Assessment

- **Breaking Changes**: NONE - All fixes are backward compatible
- **Performance Impact**: Minimal - Cache invalidation adds ~0.1ms per bulk operation (negligible)
- **Migration Required**: NONE - Drop-in replacement for v0.5.3

### Dependencies

- Requires Kailash SDK >= 0.9.21 (no change from 0.5.3)

## [0.5.3] - 2025-10-10

### Critical Bug Fixes

#### Bulk Operation Truthiness Bugs (Bugs #1-3)

- **Fixed**: Empty dict/list handling in bulk_create, bulk_update, bulk_delete operations
- **Impact**: MongoDB-style empty filter `{}` and empty data `[]` now work correctly
- **Root Causes**:
  1. **Bug #1**: BulkDeleteNode empty filter execution failure - Python truthiness check failed on empty dict
  2. **Bug #2**: BulkCreateNode "Unsupported operation" error - Missing key existence check before accessing kwargs
  3. **Bug #3**: Generic error messages - Errors not properly propagated from bulk.py to nodes.py
- **Locations**:
  - `src/dataflow/core/nodes.py:1905, 1937, 1969` (changed to key existence checks)
  - `src/dataflow/features/bulk.py:87, 131, 175` (added proper error propagation)
  - `src/dataflow/core/nodes.py:1975` (added missing await for async bulk_update)
- **Tests**: 119/119 tests passing (100% pass rate, NO REGRESSIONS)
- **Breaking**: NO - previously broken functionality now works

**Before**:

```python
# Empty filter/data failed with various errors
node = BulkDeleteNode(...)
result = await node.async_run(filter={}, confirmed=True)  # ❌ FAILED
# Error: "Unsupported bulk operation" or generic errors

node = BulkCreateNode(...)
result = await node.async_run(data=[])  # ❌ FAILED
# Error: KeyError or "Unsupported operation"
```

**After**:

```python
# Empty filter/data works correctly
node = BulkDeleteNode(...)
result = await node.async_run(filter={}, confirmed=True)  # ✅ WORKS
# Successfully processes empty filter as "match all"

node = BulkCreateNode(...)
result = await node.async_run(data=[])  # ✅ WORKS
# Successfully handles empty data gracefully
```

### Real Database Operations Implementation

- **Implemented**: Real database operations for bulk_create, bulk_update, bulk_delete
- **Impact**: All bulk operations now execute actual SQL via AsyncSQLDatabaseNode
- **Features**:
  - Real INSERT, UPDATE, DELETE SQL execution
  - Proper error propagation from database layer
  - Support for batch processing
  - Transaction-aware operations
- **Location**: `src/dataflow/features/bulk.py`
- **Tests**: Comprehensive integration tests with real PostgreSQL database

### Safety Features

- **Added**: safe_mode parameter for bulk operations (default: True for delete)
- **Added**: confirmed parameter requirement for dangerous operations
- **Added**: Empty filter validation with clear error messages
- **Impact**: Prevents accidental full-table deletion/updates

**Safety Example**:

```python
# Safe mode prevents accidental deletion
node = BulkDeleteNode(safe_mode=True)  # Default
result = await node.async_run(filter={}, confirmed=True)  # ❌ Raises error
# Error: "Empty filter would delete all records. Set safe_mode=False if intentional"

# Explicit override for intentional full-table operations
node = BulkDeleteNode(safe_mode=False)
result = await node.async_run(filter={}, confirmed=True)  # ✅ Works
```

### Test Coverage

- **Bug Reproduction Tests**: 5/5 PASSED in `tests/integration/bulk_operations/test_v052_bug_reproduction.py`
  - `test_bug_1_bulk_delete_empty_filter` - Empty filter execution
  - `test_bug_2_bulk_create_unsupported_operation` - KeyError fix
  - `test_bug_3_generic_error_messages` - Error propagation
  - `test_empty_data_handling` - Empty list handling
  - `test_error_propagation_chain` - Full error chain validation

- **Bulk Update Tests**: 8/8 PASSED in `tests/integration/bulk_operations/test_bulk_update_real_operations.py`
  - Real database UPDATE operations
  - Transaction support
  - Error handling
  - Edge cases (empty filter, no matches, etc.)

- **Unit Tests**: 36/36 PASSED
- **Integration Tests**: 70/70 PASSED
- **Total**: 119/119 tests passing (100%)
- **Infrastructure**: Real PostgreSQL testing (NO MOCKING policy)
- **Regressions**: ZERO - all existing tests continue to pass

### Enhanced

- Improved error messages with detailed context for bulk operations
- Better validation for empty filter/data edge cases
- Comprehensive debug logging for bulk operation failures
- Clear documentation of safety parameters

### Dependencies

- Requires Kailash SDK >= 0.9.21 (no change from 0.5.2)

## [0.5.2] - 2025-10-10

### Critical Bug Fixes

#### Empty Filter Bug in Bulk Operations (Bug #4)

- **Fixed**: BulkDeleteNode and BulkUpdateNode now accept empty filter `{}` for "match all" operations
- **Impact**: MongoDB-style empty filter syntax now works correctly for bulk operations
- **Root Cause**: Python truthiness check failed on empty dict (empty dict evaluates to `False`)
- **Locations**:
  - `src/dataflow/core/nodes.py:1905, 1937` (changed to key existence check `"filter" in kwargs`)
  - `src/dataflow/nodes/bulk_delete.py:153` (changed to `"filter" not in validated_inputs`)
  - `src/dataflow/nodes/bulk_update.py:162` (changed to `"filter" not in validated_inputs`)
- **Tests**: 4 regression tests + 48 bulk operation integration tests (all passing)
- **Breaking**: NO - previously broken functionality now works

**Before**:

```python
# Empty filter failed with "Unsupported bulk operation" error
node = BulkDeleteNode(...)
result = await node.async_run(filter={}, confirmed=True)  # ❌ FAILED
# Error: "Unsupported bulk operation: bulk_delete"
```

**After**:

```python
# Empty filter works as "match all" (MongoDB-style)
node = BulkDeleteNode(...)
result = await node.async_run(filter={}, confirmed=True)  # ✅ WORKS
# Successfully deletes/updates all records in table
```

**Security Note**: Empty filter `{}` means "match all records". Always use with caution:

- BulkDeleteNode has `safe_mode` enabled by default to prevent accidental full-table deletion
- Set `safe_mode=False` explicitly if you intend to delete all records
- Always use `confirmed=True` for dangerous operations

### Test Coverage

- **New Regression Tests**: 4 comprehensive tests in `tests/integration/bulk_operations/test_bulk_empty_filter_regression.py`
  - `test_bulk_delete_with_empty_filter` - Empty filter deletes all records
  - `test_bulk_update_with_empty_filter` - Empty filter updates all records
  - `test_empty_filter_vs_non_empty_filter` - Correctly distinguishes empty vs non-empty
  - `test_no_filter_parameter_still_works` - Operations without filter still work
- **Total Bulk Tests**: 48 integration tests (100% pass rate)
- **Infrastructure**: Real PostgreSQL testing (NO MOCKING policy)
- **Zero Regressions**: All existing tests pass with the fix

### Dependencies

- Requires Kailash SDK >= 0.9.21 (no change from 0.5.1)

## [0.5.1] - 2025-10-09

### Critical Bug Fixes

#### JSONB Serialization (Bug #1)

- **Fixed**: JSONB fields now use `json.dumps()` instead of `str()` for dict/list serialization
- **Impact**: Prevents PostgreSQL errors with invalid JSON syntax (single quotes vs double quotes)
- **Location**: `src/dataflow/core/nodes.py:211-216`
- **Tests**: 9 comprehensive tests in `tests/integration/test_jsonb_bug_reproduction.py`
- **Breaking**: NO - transparent fix for previously broken functionality

**Before**:

```python
str({'key': 'value'})  # → "{'key': 'value'}" (invalid JSON - single quotes)
```

**After**:

```python
json.dumps({'key': 'value'})  # → '{"key": "value"}' (valid JSON - double quotes)
```

#### DeleteNode Safety Validation (Bug #2)

- **Fixed**: DeleteNode now raises `ValueError` when no ID is provided instead of silently defaulting to `id=1`
- **Impact**: Prevents accidental data loss from unintentional deletions
- **Location**: `src/dataflow/core/nodes.py:1437-1443`
- **Tests**: 7 comprehensive tests in `tests/integration/core_engine/test_delete_node_validation.py`
- **Breaking**: YES - intentional security improvement

**BREAKING CHANGE**: DeleteNode now requires explicit `id` or `record_id` parameter

**Migration Required**:

```python
# Before (DANGEROUS - silently deleted id=1):
workflow.add_node("ProductDeleteNode", "delete", {})

# After (SAFE - must provide explicit ID):
workflow.add_node("ProductDeleteNode", "delete", {"id": 5})
# or
workflow.add_node("ProductDeleteNode", "delete", {"record_id": 5})
```

#### Reserved Parameter Names (Bug #3)

- **Fixed**: Complete namespace separation between node metadata and user parameters
- **Impact**: Users can now freely use 'id' as a parameter name (string OR integer types)
- **Locations**:
  - Core SDK: `src/kailash/workflow/graph.py` (inject `_node_id` instead of `id`)
  - Core SDK: `src/kailash/nodes/base.py` (use `_node_id` internally, add `id` property)
  - DataFlow: `src/dataflow/core/nodes.py` (accept integer IDs, dynamic SQL generation)
- **Tests**: 5 comprehensive tests in `tests/integration/test_bug_3_reserved_fields_fix.py`
- **Breaking**: NO - backward compatible via property alias

**Before**:

```python
# Users couldn't use 'id' parameter due to namespace collision
workflow.add_node("SessionCreateNode", "create", {
    "session_id": "sess-123",  # Had to use alternative field name
    "user_id": "user-456"
})
```

**After**:

```python
# Users can freely use 'id' parameter (string or integer)
workflow.add_node("SessionCreateNode", "create", {
    "id": "sess-123",  # Now works!
    "user_id": "user-456"
})
```

### Test Coverage

- **Total Tests**: 21 comprehensive integration tests (100% pass rate)
- **Infrastructure**: Real PostgreSQL testing (NO MOCKING policy)
- **Verification**: 1,420+ existing tests verified with no new regressions

### Enhanced

- Dynamic SQL generation for flexible parameter handling
- Improved error messages for DeleteNode validation
- Better namespace separation between framework and user code

### Dependencies

- Requires Kailash SDK >= 0.9.21 (updated from >= 0.9.16)

## [0.4.0] - 2025-08-04

### Major Features

- **TDD Foundation Implementation**: Complete Test-Driven Development infrastructure with <100ms test execution
  - TDD-aware connection management for maximum performance and test isolation
  - Enhanced test fixtures and isolation mechanisms
  - Performance optimization with sub-100ms test execution through connection reuse
  - Comprehensive TDD documentation and examples

- **Dynamic Model Registration**: Enhanced model registration system with runtime discovery
  - Dynamic schema discovery and model reconstruction capabilities
  - Improved existing database integration workflows
  - Better model registry management for multi-application scenarios

### Enhanced Testing Infrastructure

- **4,000+ Test Milestone**: Comprehensive testing coverage with 4,072 passing tier 1 tests
- **Test Organization**: Restructured test suite with clear separation of unit, integration, and E2E tests
- **Performance Optimization**: Test execution optimized for development workflow efficiency
- **Real Infrastructure Testing**: Enhanced PostgreSQL and SQLite integration testing

### Fixed

- Merge conflict resolution with proper initialize() method implementation
- Import order corrections across test modules
- Enhanced error handling in migration systems
- Improved connection pool management in test environments

### Changed

- Updated Kailash SDK dependency to >=0.9.11 for latest compatibility
- Enhanced documentation structure with comprehensive TDD guidance
- Improved code formatting and linting compliance
- Better test isolation and cleanup mechanisms

### Developer Experience

- Complete TDD workflow implementation for rapid development cycles
- Enhanced debugging capabilities with comprehensive test coverage
- Improved error messages and validation feedback
- Streamlined development setup and testing procedures

## [0.3.3] - 2025-07-31

### Fixed

- Critical connection string parsing issues with special characters in passwords
- Database URL parsing now uses proper urllib.parse for robust handling
- Password parsing bug where '#' character caused int() conversion errors
- Connection parameter validation for complex database URLs

### Enhanced

- ConnectionParser class with improved URL parsing capabilities
- DatabaseRegistry with better connection handling and error reporting
- MultiDatabase adapter with enhanced connection validation
- Better error messages for connection parsing failures

### Added

- Comprehensive bug reproduction tools and analysis scripts
- Enhanced connection string parsing test coverage
- Support for URL-encoded special characters in passwords
- Better debugging utilities for connection issues

### Dependencies

- Requires Kailash SDK >= 0.9.4 (updated from >= 0.9.2)
- All other dependencies remain compatible

## [0.3.2] - 2025-07-30

### Fixed

- Minor bug fixes and improvements
- Enhanced stability for production deployments

## [0.3.1] - 2025-07-22

### Added

- Comprehensive release notes documenting all improvements
- Enhanced parameter validation error messages
- Redis integration tests with cache operations
- Performance benchmarks for bulk operations

### Changed

- Improved test pass rate from ~40% to 90.7% (330/364 tests passing)
- Zero failing tests - all tests now pass or are properly skipped
- Enhanced documentation for parameter validation patterns
- Updated CLAUDE.md files with debugging guidance

### Fixed

- Template string parameter validation - `${}` syntax now properly rejected
- DateTime format handling - use native datetime objects
- Floating point precision comparisons in PostgreSQL tests
- Bulk operations assertion handling for metadata responses
- Circuit breaker parameter names (recovery_timeout, half_open_requests)
- Multi-tenancy Row Level Security (RLS) tests
- Transaction management DataFlow context passing

### Developer Experience

- Added debugging section to root CLAUDE.md for parameter errors
- Direct links to parameter solution guides
- Moved parameter validation to step 2 in Multi-Step Strategy
- Clear migration guide for parameter passing patterns

## [0.3.0] - 2025-07-21

### Added

- DataFlow test utilities (`DataFlowTestUtils`) for clean database management
- Migration-based table cleanup functionality
- Support for direct node execution pattern in tests
- Comprehensive test coverage improvements

### Changed

- Replaced all psql command line usage with DataFlow components
- Updated all e2e tests to use DataFlow's own database operations
- Improved test reliability by removing external tool dependencies
- Enhanced integration test structure for better maintainability

### Fixed

- Database cleanup issues in integration tests
- Test failures due to missing psql command
- DatabaseConfig parameter compatibility issues
- Connection management in concurrent test scenarios

### Developer Experience

- Simplified test database setup and teardown
- Better error messages for database operations
- Consistent use of DataFlow patterns across all tests

## [0.2.0] - 2025-07-20

### Breaking Changes

- Updated Nexus integration imports from `from kailash.nexus import create_nexus` to `from nexus import Nexus`
- Requires Kailash SDK >= 0.8.5 (previously >= 0.8.3)

### Fixed

- Version mismatch between setup.py and **init**.py (now consistently 0.2.0)
- Gateway integration now uses correct Nexus import pattern

### Changed

- Updated documentation examples to use new Nexus import pattern
- SQL injection test scenarios updated to use new import

## [0.1.1] - Previous Release

- Initial release with modular architecture
- Enterprise features including bulk operations and multi-tenancy
- MongoDB-style query API
- Zero-configuration setup
