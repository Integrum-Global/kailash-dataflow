# DataFlow Database Expansion Strategy
**Version**: 1.0
**Date**: 2025-10-21
**Status**: Strategic Analysis

## Executive Summary

DataFlow v0.5.6 has achieved 100% feature parity across PostgreSQL, MySQL, and SQLite with a proven adapter pattern. This document analyzes the success pattern and provides a comprehensive roadmap for expanding to document, vector, graph, time-series, and key-value databases.

**Key Recommendations**:
1. **MongoDB First**: Highest ROI, mature async driver, fits 9-node pattern
2. **Tiered Adapter Architecture**: Introduce specialized base classes for non-relational patterns
3. **Phased Rollout**: 10 databases over 3 phases based on market demand and technical feasibility

---

## 1. Pattern Analysis: What Makes the Current Adapter Successful?

### 1.1 Core Success Factors

#### Clear Abstraction Boundaries (base.py:16-128)
```python
class DatabaseAdapter(ABC):
    """Abstract base class for database adapters."""

    # ✅ STRENGTH: Clean separation of concerns
    @abstractmethod
    async def connect(self) -> None

    @abstractmethod
    async def execute_query(self, query: str, params: List[Any] = None) -> List[Dict]

    @abstractmethod
    async def execute_transaction(self, queries: List[Tuple[str, List[Any]]]) -> List[Any]

    @abstractmethod
    async def get_table_schema(self, table_name: str) -> Dict[str, Dict]
```

**Why This Works**:
- **Async-first design**: All operations use async/await (base.py:63-82)
- **Connection pooling abstraction**: Shared pattern across all adapters
- **Query parameterization**: Database-specific parameter style conversion (base.py:109-114)
- **Schema introspection**: Standardized schema discovery (base.py:85-97)

#### Database-Specific Optimizations Without Breaking Pattern

**PostgreSQL** (postgresql.py:16-424):
- Uses `asyncpg` for native performance (postgresql.py:49)
- Parameter style: `$1, $2, $3` conversion (postgresql.py:307-322)
- Features: JSON, arrays, CTEs, window functions (postgresql.py:290-305)
- Connection pool: `asyncpg.create_pool()` (postgresql.py:55)

**MySQL** (mysql.py:22-525):
- Uses `aiomysql` with DictCursor (mysql.py:110)
- Parameter style: `%s` conversion (mysql.py:325-335)
- Features: JSON (5.7+), window functions (8.0+) (mysql.py:307-323)
- Storage engine awareness: InnoDB detection (mysql.py:400-439)

**SQLite** (sqlite.py:82-854):
- Uses `aiosqlite` with connection pooling (sqlite.py:251-285)
- Parameter style: `?` (no conversion needed) (sqlite.py:488-496)
- Enterprise features: WAL mode, custom pool, performance monitoring (sqlite.py:115-196)
- Performance optimization: 64MB cache, memory-mapped I/O (sqlite.py:133-192)

#### Automatic Node Generation Pattern (core/nodes.py:81-135)
```python
def generate_crud_nodes(self, model_name: str, fields: Dict[str, Any]):
    """Generate CRUD workflow nodes for a model."""
    nodes = {
        f"{model_name}CreateNode": self._create_node_class(model_name, "create", fields),
        f"{model_name}ReadNode": self._create_node_class(model_name, "read", fields),
        f"{model_name}UpdateNode": self._create_node_class(model_name, "update", fields),
        f"{model_name}DeleteNode": self._create_node_class(model_name, "delete", fields),
        f"{model_name}ListNode": self._create_node_class(model_name, "list", fields),
    }
```

**9-Node Pattern**:
1. CreateNode - Single record insert
2. ReadNode - Single record read by ID
3. UpdateNode - Single record update
4. DeleteNode - Single record delete
5. ListNode - Query multiple records
6. BulkCreateNode - Batch insert
7. BulkUpdateNode - Batch update
8. BulkDeleteNode - Batch delete
9. BulkUpsertNode - Batch upsert (INSERT ON CONFLICT)

**File References**:
- Core nodes: `src/dataflow/core/nodes.py:81-135`
- Bulk operations: `src/dataflow/nodes/bulk_*.py`

### 1.2 Key Architectural Patterns

#### Connection String Parsing (adapters/connection_parser.py)
- Handles special characters in passwords
- Extracts scheme, host, port, database, credentials
- Query parameter parsing for SSL modes, charsets, etc.

#### Transaction Management
- **PostgreSQL**: Context manager with `asyncpg.Transaction` (postgresql.py:398-424)
- **MySQL**: Context manager with `BEGIN/COMMIT/ROLLBACK` (mysql.py:501-525)
- **SQLite**: WAL mode with savepoints (sqlite.py:808-854)

#### Feature Detection (supports_feature method)
Each adapter declares feature support:
- PostgreSQL: Arrays, hstore, fulltext, spatial indexes
- MySQL: JSON (5.7+), CTEs (8.0+), fulltext
- SQLite: FTS5, JSON1, CTEs, upsert

---

## 2. Database Categorization and Architectural Implications

### 2.1 Document Databases

| Database | Market Share | Python Driver | Async Support | Fit Score |
|----------|-------------|---------------|---------------|-----------|
| **MongoDB** | 45% NoSQL market | `motor` (official) | ✅ Native async | 9/10 |
| CouchDB | 5% | `aiocouch` | ✅ | 6/10 |
| RethinkDB | 2% | `rethinkdb-async` | ✅ | 5/10 |

#### Architectural Challenges

**Schema Operations** - What breaks?
```python
# Current SQL pattern
async def get_table_schema(self, table_name: str) -> Dict[str, Dict]:
    query = "SELECT column_name, data_type FROM information_schema.columns WHERE table_name = $1"
```

**Document DB equivalent** (MongoDB):
```python
async def get_collection_schema(self, collection_name: str) -> Dict[str, Dict]:
    # MongoDB has no enforced schema - infer from documents
    pipeline = [
        {"$limit": 1000},  # Sample documents
        {"$project": {"fields": {"$objectToArray": "$$ROOT"}}}
    ]
    # Infer types from actual data
```

**CRUD Translation**:
- ✅ CREATE → `collection.insert_one(document)`
- ✅ READ → `collection.find_one({"_id": ObjectId(...)})`
- ✅ UPDATE → `collection.update_one(filter, {"$set": updates})`
- ✅ DELETE → `collection.delete_one({"_id": ObjectId(...)})`
- ✅ LIST → `collection.find(query).to_list(length=100)`

**Bulk Operations**:
- ✅ BulkCreate → `collection.insert_many(documents)`
- ✅ BulkUpdate → `collection.bulk_write([UpdateOne(...), ...])`
- ✅ BulkDelete → `collection.delete_many(filter)`
- ✅ BulkUpsert → `collection.update_many(filter, update, upsert=True)`

**Unique Operations to Add**:
- **AggregateNode**: MongoDB aggregation pipelines
- **IndexNode**: Create/manage indexes
- **LookupNode**: `$lookup` for joins across collections
- **UnwindNode**: `$unwind` for array flattening

**Fit Score**: 9/10 - Excellent fit with minor adaptations

---

### 2.2 Vector Databases

| Database | License | Python Driver | Async Support | Production Ready | Fit Score |
|----------|---------|---------------|---------------|------------------|-----------|
| **Milvus** | Apache 2.0 | `pymilvus` | ⚠️ Partial | ✅ | 8/10 |
| **Weaviate** | BSD-3 | `weaviate-client` | ✅ | ✅ | 8/10 |
| **Chroma** | Apache 2.0 | `chromadb` | ✅ | ⚠️ Beta | 7/10 |
| **Qdrant** | Apache 2.0 | `qdrant-client` | ✅ | ✅ | 8/10 |
| **pgvector** | PostgreSQL | `asyncpg` | ✅ | ✅ | 10/10 |

#### Architectural Challenges

**Schema Operations** - Collections, not tables:
```python
# Vector DB pattern (Milvus example)
async def create_collection(self, collection_name: str, schema: Dict[str, Dict]) -> None:
    from pymilvus import CollectionSchema, FieldSchema, DataType

    fields = []
    for field_name, field_info in schema.items():
        if field_info.get("is_vector"):
            fields.append(FieldSchema(
                name=field_name,
                dtype=DataType.FLOAT_VECTOR,
                dim=field_info["dimension"]
            ))
        else:
            fields.append(FieldSchema(name=field_name, dtype=field_info["type"]))

    schema = CollectionSchema(fields=fields, description=collection_name)
    collection = Collection(name=collection_name, schema=schema)
```

**CRUD Translation**:
- ✅ CREATE → `collection.insert(entities)` with embeddings
- ⚠️ READ → Not by ID, but by vector similarity
- ⚠️ UPDATE → Many vector DBs don't support updates (delete+insert)
- ✅ DELETE → `collection.delete(expr="id in [...]")`
- ⚠️ LIST → Semantic search, not traditional listing

**Unique Operations Required**:
- **EmbedNode**: Generate embeddings from text/images (integration with Kaizen)
- **SimilaritySearchNode**: KNN/ANN search by vector
- **HybridSearchNode**: Combine vector + metadata filtering
- **IndexBuildNode**: Build HNSW/IVF indexes

**Fit Score Analysis**:
- **Milvus**: 8/10 - Most mature, but partial async support
- **Weaviate**: 8/10 - Good async, GraphQL interface
- **Qdrant**: 8/10 - Excellent async, gRPC and REST
- **pgvector**: 10/10 - **RECOMMENDED FIRST** - Leverage existing PostgreSQL adapter!

**pgvector Strategy** (CRITICAL):
```python
# Extend existing PostgreSQLAdapter for vector operations
class PostgreSQLVectorAdapter(PostgreSQLAdapter):
    async def create_vector_index(self, table_name: str, column_name: str, distance: str = "cosine"):
        # CREATE INDEX ON table USING ivfflat (embedding vector_cosine_ops)

    async def vector_search(self, table_name: str, query_vector: List[float], k: int = 10):
        # SELECT * FROM table ORDER BY embedding <=> $1 LIMIT $2
```

**Recommendation**: Implement pgvector FIRST as extension of PostgreSQL adapter (LOW EFFORT, HIGH VALUE).

---

### 2.3 Graph Databases

| Database | License | Python Driver | Async Support | Production Ready | Fit Score |
|----------|---------|---------------|---------------|------------------|-----------|
| **Neo4j** | GPL/Commercial | `neo4j` | ✅ | ✅ | 7/10 |
| **ArangoDB** | Apache 2.0 | `python-arango` | ⚠️ Limited | ✅ | 6/10 |
| **JanusGraph** | Apache 2.0 | `gremlinpython` | ⚠️ | ✅ | 5/10 |
| **Dgraph** | Apache 2.0 | `pydgraph` | ✅ | ✅ | 7/10 |

#### Architectural Challenges

**Schema Operations** - Nodes, edges, properties:
```python
# Graph DB pattern (Neo4j Cypher)
async def get_graph_schema(self) -> Dict[str, Any]:
    # CALL db.schema.visualization()
    # Returns node labels, relationship types, property keys
```

**CRUD Translation** - Completely different model:
- ⚠️ CREATE → `CREATE (n:Person {name: $name})` (nodes) or `CREATE (a)-[:KNOWS]->(b)` (edges)
- ⚠️ READ → `MATCH (n:Person {id: $id}) RETURN n`
- ⚠️ UPDATE → `MATCH (n:Person {id: $id}) SET n.name = $name`
- ⚠️ DELETE → `MATCH (n:Person {id: $id}) DETACH DELETE n` (removes edges too)
- ⚠️ LIST → `MATCH (n:Person) RETURN n LIMIT 100`

**Unique Operations Required**:
- **TraverseNode**: `MATCH (a)-[:FRIEND*1..3]->(b)` (multi-hop traversal)
- **PathFindNode**: Shortest path algorithms
- **CentralityNode**: PageRank, betweenness centrality
- **CommunityDetectionNode**: Louvain, label propagation

**Fit Score**: 5-7/10 - **Significant architecture divergence**. Graph queries don't map cleanly to 9-node CRUD pattern.

**Recommendation**: Graph databases require **GraphAdapter** base class with different node pattern.

---

### 2.4 Time-Series Databases

| Database | License | Python Driver | Async Support | Production Ready | Fit Score |
|----------|---------|---------------|---------------|------------------|-----------|
| **TimescaleDB** | Apache 2.0 | `asyncpg` (PostgreSQL) | ✅ | ✅ | 10/10 |
| **InfluxDB** | MIT | `influxdb-client` | ✅ | ✅ | 7/10 |

#### Architectural Challenges

**Schema Operations** - Hypertables, retention policies:
```python
# TimescaleDB pattern (extends PostgreSQL)
async def create_hypertable(self, table_name: str, time_column: str):
    await self.execute_query(f"SELECT create_hypertable('{table_name}', '{time_column}')")
```

**CRUD Translation**:
- ✅ CREATE → INSERT with timestamps
- ⚠️ READ → Time-range queries, not by ID
- ⚠️ UPDATE → Discouraged in time-series (append-only)
- ⚠️ DELETE → Retention policies, not individual deletes
- ✅ LIST → Time-range queries with aggregations

**Unique Operations Required**:
- **TimeRangeNode**: Query by time window
- **DownsampleNode**: Time-based aggregation
- **RetentionPolicyNode**: Auto-delete old data
- **ContinuousAggregateNode**: Real-time rollups

**Fit Score Analysis**:
- **TimescaleDB**: 10/10 - **RECOMMENDED** - Extends PostgreSQL adapter!
- **InfluxDB**: 7/10 - Requires specialized adapter

**Recommendation**: TimescaleDB FIRST as extension of PostgreSQL adapter (LOW EFFORT, HIGH VALUE).

---

### 2.5 Key-Value Stores

| Database | License | Python Driver | Async Support | Production Ready | Fit Score |
|----------|---------|---------------|---------------|------------------|-----------|
| **Redis** | BSD-3 | `redis-py` | ✅ | ✅ | 6/10 |
| **DynamoDB** | AWS Managed | `aioboto3` | ✅ | ✅ | 5/10 |

#### Architectural Challenges

**Schema Operations** - No schema:
```python
# Redis has no schema
async def get_schema(self) -> Dict:
    return {}  # Key-value stores are schemaless
```

**CRUD Translation**:
- ✅ CREATE → `SET key value`
- ✅ READ → `GET key`
- ✅ UPDATE → `SET key new_value` (overwrites)
- ✅ DELETE → `DEL key`
- ⚠️ LIST → `KEYS pattern` (discouraged in production) or `SCAN`

**Unique Operations Required**:
- **ExpireNode**: Set TTL on keys
- **IncrementNode**: Atomic counter operations
- **PubSubNode**: Publish/subscribe patterns
- **StreamNode**: Redis Streams for event sourcing

**Fit Score**: 5-6/10 - **Poor fit for DataFlow's model-centric pattern**. Better suited for caching layer integration.

**Recommendation**: Integrate Redis as **DataFlow caching layer**, not primary database.

---

## 3. MongoDB Priority Analysis

### 3.1 Why MongoDB First?

**Market Demand**:
- 45% of NoSQL market share
- 30+ million downloads/month of `pymongo`
- Primary choice for document-based applications
- Strong Python ecosystem with `motor` (async driver)

**Technical Feasibility**:
- ✅ Mature async driver: `motor` (official, actively maintained)
- ✅ Excellent fit for 9-node pattern (8/10 mapping)
- ✅ Collection-based model maps to DataFlow's table-centric design
- ✅ Aggregation pipelines extend naturally

**Implementation Effort**: MEDIUM
- Base adapter: ~400 lines (similar to MySQL)
- 9 core nodes: Leverage existing pattern
- Additional nodes: 4-5 MongoDB-specific (aggregation, indexing)

### 3.2 MongoDB Adapter Design

```python
# adapters/mongodb.py

import motor.motor_asyncio
from typing import Any, Dict, List, Tuple
from .base import DatabaseAdapter

class MongoDBAdapter(DatabaseAdapter):
    """MongoDB database adapter using motor (async driver)."""

    @property
    def database_type(self) -> str:
        return "mongodb"

    @property
    def default_port(self) -> int:
        return 27017

    async def connect(self) -> None:
        """Establish MongoDB connection using motor."""
        self.client = motor.motor_asyncio.AsyncIOMotorClient(
            self.connection_string,
            maxPoolSize=self.pool_size,
            minPoolSize=self.pool_size // 2,
            serverSelectionTimeoutMS=self.pool_timeout * 1000
        )
        self.db = self.client[self.database]
        self.is_connected = True

    async def execute_query(self, query: Dict[str, Any], collection: str = None) -> List[Dict]:
        """Execute MongoDB query (find operation)."""
        coll = self.db[collection or self._current_collection]
        cursor = coll.find(query)
        return await cursor.to_list(length=None)

    async def execute_transaction(self, operations: List[Tuple[str, Dict]]) -> List[Any]:
        """Execute multiple operations in MongoDB transaction."""
        async with await self.client.start_session() as session:
            async with session.start_transaction():
                results = []
                for op_type, op_data in operations:
                    if op_type == "insert":
                        result = await self.db[op_data["collection"]].insert_one(
                            op_data["document"], session=session
                        )
                        results.append(result)
                    # ... other operations
                return results

    async def get_collection_schema(self, collection_name: str) -> Dict[str, Dict]:
        """Infer schema from collection documents (MongoDB is schemaless)."""
        pipeline = [
            {"$limit": 1000},  # Sample 1000 documents
            {"$project": {"fields": {"$objectToArray": "$$ROOT"}}},
            {"$unwind": "$fields"},
            {"$group": {
                "_id": "$fields.k",
                "types": {"$addToSet": {"$type": "$fields.v"}}
            }}
        ]

        schema_docs = await self.db[collection_name].aggregate(pipeline).to_list(None)

        # Convert to DataFlow schema format
        schema = {}
        for doc in schema_docs:
            field_name = doc["_id"]
            types = doc["types"]
            schema[field_name] = {
                "type": self._infer_python_type(types),
                "nullable": True,  # MongoDB fields are always optional
                "primary_key": field_name == "_id"
            }

        return schema

    async def create_collection(self, collection_name: str, schema: Dict[str, Dict]) -> None:
        """Create MongoDB collection with optional schema validation."""
        # MongoDB creates collections lazily, but we can set up validation
        validator = self._build_json_schema_validator(schema)
        await self.db.create_collection(
            collection_name,
            validator={"$jsonSchema": validator}
        )

    async def drop_collection(self, collection_name: str) -> None:
        """Drop MongoDB collection."""
        await self.db[collection_name].drop()

    def supports_feature(self, feature: str) -> bool:
        """Check MongoDB feature support."""
        features = {
            "transactions": True,  # MongoDB 4.0+
            "aggregation": True,
            "text_search": True,
            "geospatial": True,
            "schema_validation": True,  # Optional
            "change_streams": True,
            "gridfs": True,  # For large files
        }
        return features.get(feature, False)
```

### 3.3 MongoDB-Specific Nodes

**AggregateNode** - MongoDB aggregation pipelines:
```python
class UserAggregateNode(AsyncNode):
    """Execute MongoDB aggregation pipeline."""

    async def execute_async(self, **inputs):
        pipeline = inputs.get("pipeline", [])
        collection = self.dataflow_instance.db[self.model_name.lower()]

        results = await collection.aggregate(pipeline).to_list(None)
        return {"results": results}
```

**IndexNode** - Create/manage indexes:
```python
class UserIndexNode(AsyncNode):
    """Create index on MongoDB collection."""

    async def execute_async(self, **inputs):
        field = inputs["field"]
        index_type = inputs.get("index_type", "ascending")  # or "text", "geospatial"

        collection = self.dataflow_instance.db[self.model_name.lower()]

        if index_type == "text":
            await collection.create_index([(field, "text")])
        elif index_type == "geospatial":
            await collection.create_index([(field, "2dsphere")])
        else:
            await collection.create_index([(field, 1)])  # ascending

        return {"index_created": True, "field": field}
```

### 3.4 Implementation Roadmap

**Phase 1** (Week 1-2): Core Adapter
- [ ] `MongoDBAdapter` class with motor integration
- [ ] Connection pooling and lifecycle management
- [ ] Basic CRUD operations (insert_one, find_one, update_one, delete_one)
- [ ] Transaction support (MongoDB 4.0+ with replica sets)

**Phase 2** (Week 3): Node Generation
- [ ] Extend `NodeGenerator` to detect MongoDB instances
- [ ] Generate 9 core nodes (Create, Read, Update, Delete, List, BulkCreate, BulkUpdate, BulkDelete, BulkUpsert)
- [ ] Map DataFlow models to MongoDB collections
- [ ] Schema inference from documents

**Phase 3** (Week 4): MongoDB-Specific Features
- [ ] AggregateNode for pipelines
- [ ] IndexNode for index management
- [ ] LookupNode for `$lookup` joins
- [ ] UnwindNode for array operations

**Phase 4** (Week 5): Testing & Documentation
- [ ] Unit tests (Tier 1): Adapter methods
- [ ] Integration tests (Tier 2): Real MongoDB instance
- [ ] E2E tests (Tier 3): Full workflows
- [ ] Documentation and examples

**Estimated Effort**: 5 weeks, 1 developer

---

## 4. Vector Database Deep Dive

### 4.1 Production-Ready Open Source Options

#### pgvector (PostgreSQL Extension) - **RECOMMENDED FIRST**

**Pros**:
- ✅ Leverage existing PostgreSQL adapter (minimal new code)
- ✅ Proven stability (PostgreSQL foundation)
- ✅ ACID transactions with vectors
- ✅ Combine relational + vector data
- ✅ Easy deployment (single database)

**Cons**:
- ⚠️ Limited to PostgreSQL users
- ⚠️ Smaller index types than specialized DBs (IVFFlat, HNSW)

**Implementation**:
```python
# Extend existing PostgreSQLAdapter
class PostgreSQLVectorAdapter(PostgreSQLAdapter):
    """PostgreSQL with pgvector extension."""

    async def ensure_pgvector_extension(self):
        await self.execute_query("CREATE EXTENSION IF NOT EXISTS vector")

    async def create_vector_column(self, table_name: str, column_name: str, dimensions: int):
        await self.execute_query(
            f"ALTER TABLE {table_name} ADD COLUMN {column_name} vector({dimensions})"
        )

    async def create_vector_index(self, table_name: str, column_name: str,
                                   index_type: str = "ivfflat", distance: str = "cosine"):
        if index_type == "ivfflat":
            # IVFFlat index for approximate nearest neighbor
            await self.execute_query(
                f"CREATE INDEX ON {table_name} USING ivfflat ({column_name} vector_cosine_ops)"
            )
        elif index_type == "hnsw":
            # HNSW index for better accuracy
            await self.execute_query(
                f"CREATE INDEX ON {table_name} USING hnsw ({column_name} vector_cosine_ops)"
            )

    async def vector_search(self, table_name: str, query_vector: List[float],
                           k: int = 10, distance: str = "cosine") -> List[Dict]:
        # <=> is cosine distance operator in pgvector
        # <-> is L2 distance, <#> is inner product
        operator = {"cosine": "<=>", "l2": "<->", "inner": "<#>"}[distance]

        query = f"""
            SELECT *, {column_name} {operator} $1 AS distance
            FROM {table_name}
            ORDER BY distance
            LIMIT $2
        """
        return await self.execute_query(query, [query_vector, k])
```

**Effort**: LOW (1-2 weeks) - Extends existing adapter
**Value**: HIGH - Immediate vector support for PostgreSQL users

---

#### Milvus (Standalone Vector DB)

**Pros**:
- ✅ Purpose-built for vectors (high performance)
- ✅ Supports 11+ index types (HNSW, IVF, ANNOY, etc.)
- ✅ Hybrid search (vector + scalar filtering)
- ✅ Horizontal scalability

**Cons**:
- ⚠️ Async support is partial (`pymilvus` mostly sync)
- ⚠️ Separate infrastructure (not embedded)
- ⚠️ Eventual consistency model

**Implementation**:
```python
# adapters/milvus.py

from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType
from typing import Any, Dict, List

class MilvusAdapter(DatabaseAdapter):
    """Milvus vector database adapter."""

    @property
    def database_type(self) -> str:
        return "milvus"

    async def connect(self) -> None:
        # Milvus uses synchronous connection (limitation)
        connections.connect(
            alias="default",
            host=self.host,
            port=self.port or 19530
        )
        self.is_connected = True

    async def create_collection(self, collection_name: str, schema: Dict[str, Dict]) -> None:
        fields = []

        # Milvus requires explicit primary key
        for field_name, field_info in schema.items():
            if field_info.get("primary_key"):
                fields.append(FieldSchema(
                    name=field_name,
                    dtype=DataType.INT64,
                    is_primary=True,
                    auto_id=True
                ))
            elif field_info.get("is_vector"):
                fields.append(FieldSchema(
                    name=field_name,
                    dtype=DataType.FLOAT_VECTOR,
                    dim=field_info["dimension"]
                ))
            else:
                # Scalar field
                dtype_map = {
                    "str": DataType.VARCHAR,
                    "int": DataType.INT64,
                    "float": DataType.FLOAT,
                    "bool": DataType.BOOL
                }
                fields.append(FieldSchema(
                    name=field_name,
                    dtype=dtype_map.get(field_info["type"], DataType.VARCHAR),
                    max_length=field_info.get("max_length", 65535)
                ))

        milvus_schema = CollectionSchema(fields=fields, description=collection_name)
        collection = Collection(name=collection_name, schema=milvus_schema)

    async def vector_search(self, collection_name: str, query_vector: List[float],
                           k: int = 10, metric: str = "L2") -> List[Dict]:
        collection = Collection(collection_name)
        collection.load()  # Load into memory

        search_params = {
            "metric_type": metric,  # L2, IP (inner product), COSINE
            "params": {"nprobe": 10}  # IVF search parameter
        }

        results = collection.search(
            data=[query_vector],
            anns_field="embedding",  # Vector field name
            param=search_params,
            limit=k,
            output_fields=["*"]  # Return all fields
        )

        return [{"id": hit.id, "distance": hit.distance, **hit.entity._row_data}
                for hit in results[0]]
```

**Effort**: MEDIUM (3-4 weeks) - New adapter + async wrapper
**Value**: MEDIUM - For users needing specialized vector DB

---

#### Qdrant (Modern Vector DB)

**Pros**:
- ✅ Excellent async support (`qdrant-client` is async-first)
- ✅ REST + gRPC APIs (flexible)
- ✅ Embedded mode (no separate server needed)
- ✅ Excellent filtering and hybrid search

**Cons**:
- ⚠️ Smaller ecosystem than Milvus
- ⚠️ Less mature (released 2021)

**Implementation**:
```python
# adapters/qdrant.py

from qdrant_client import AsyncQdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

class QdrantAdapter(DatabaseAdapter):
    """Qdrant vector database adapter (async-first)."""

    @property
    def database_type(self) -> str:
        return "qdrant"

    async def connect(self) -> None:
        # Qdrant supports embedded mode or client-server
        if self.connection_string == ":memory:":
            self.client = AsyncQdrantClient(":memory:")
        else:
            self.client = AsyncQdrantClient(
                host=self.host,
                port=self.port or 6333,
                api_key=self.password  # Optional
            )
        self.is_connected = True

    async def create_collection(self, collection_name: str, schema: Dict[str, Dict]) -> None:
        # Find vector field
        vector_field = next((k for k, v in schema.items() if v.get("is_vector")), None)
        if not vector_field:
            raise ValueError("Qdrant requires at least one vector field")

        dimension = schema[vector_field]["dimension"]
        distance = schema[vector_field].get("distance", "Cosine")

        await self.client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=dimension,
                distance=Distance.COSINE if distance == "Cosine" else Distance.EUCLID
            )
        )

    async def vector_search(self, collection_name: str, query_vector: List[float],
                           k: int = 10, filter: Dict = None) -> List[Dict]:
        results = await self.client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=k,
            query_filter=filter  # Qdrant's powerful filtering
        )

        return [{"id": hit.id, "score": hit.score, **hit.payload} for hit in results]
```

**Effort**: MEDIUM (2-3 weeks) - Clean async implementation
**Value**: MEDIUM-HIGH - Best async experience

---

### 4.2 Vector Operations Integration

**How vector ops fit DataFlow**:

**Option A: Replace CRUD nodes with vector-specific nodes**
```python
# Instead of UserReadNode, generate UserVectorSearchNode
@db.model
class Document:
    id: str
    content: str
    embedding: Vector(dimension=1536)  # New type annotation

# Auto-generated nodes:
# - DocumentCreateNode (with embedding generation)
# - DocumentVectorSearchNode (similarity search)
# - DocumentHybridSearchNode (vector + metadata filters)
# - DocumentUpdateNode (update embeddings)
# - DocumentDeleteNode (standard)
```

**Option B: Additional nodes alongside CRUD**
```python
# Keep standard CRUD nodes, add vector-specific operations
# - DocumentCreateNode (standard insert)
# - DocumentReadNode (standard read by ID)
# - DocumentVectorSearchNode (NEW - similarity search)
# - DocumentReindexNode (NEW - rebuild vector index)
# - DocumentClusterNode (NEW - k-means clustering)
```

**Recommendation**: **Option B** - Preserve CRUD pattern, add specialized vector nodes. More flexible and maintains consistency.

---

### 4.3 Integration with Kaizen (AI Framework)

**Embedding Generation** - Integrate with Kaizen's AI agents:
```python
# kaizen integration for automatic embeddings
from kaizen.agents import EmbeddingAgent

class DocumentCreateNode(AsyncNode):
    async def execute_async(self, **inputs):
        content = inputs["content"]

        # Generate embedding using Kaizen agent
        embedding_agent = EmbeddingAgent(model="text-embedding-3-small")
        embedding = await embedding_agent.embed(content)

        # Store with vector
        inputs["embedding"] = embedding

        # Insert into vector DB
        await self.dataflow_instance.adapter.insert(
            collection="documents",
            data=inputs
        )
```

**Recommendation**: Tight integration between DataFlow (storage) and Kaizen (AI processing) for seamless RAG workflows.

---

## 5. Graph Database Deep Dive

### 5.1 Production-Ready Options

| Database | Query Language | Async Driver | Fit Score | Recommendation |
|----------|---------------|--------------|-----------|----------------|
| **Neo4j** | Cypher | `neo4j` (async) | 7/10 | Best choice |
| **ArangoDB** | AQL | `python-arango` (limited async) | 6/10 | Multi-model DB |
| **JanusGraph** | Gremlin | `gremlinpython` (partial async) | 5/10 | Complex setup |
| **Dgraph** | GraphQL+- | `pydgraph` (async) | 7/10 | Good alternative |

### 5.2 Neo4j Implementation Strategy

**Challenge**: Graph queries don't map to CRUD pattern.

**Solution**: Create **GraphAdapter** base class with graph-specific node pattern.

```python
# adapters/graph_base.py

from abc import abstractmethod
from .base import DatabaseAdapter

class GraphAdapter(DatabaseAdapter):
    """Base class for graph database adapters."""

    @abstractmethod
    async def create_node(self, label: str, properties: Dict[str, Any]) -> Any:
        """Create a graph node."""
        pass

    @abstractmethod
    async def create_edge(self, from_id: Any, to_id: Any,
                         relationship: str, properties: Dict = None) -> Any:
        """Create a graph edge/relationship."""
        pass

    @abstractmethod
    async def traverse(self, start_id: Any, pattern: str, depth: int = 3) -> List[Dict]:
        """Traverse graph from starting node."""
        pass

    @abstractmethod
    async def find_path(self, from_id: Any, to_id: Any,
                       relationship: str = None) -> List[List[Any]]:
        """Find path(s) between two nodes."""
        pass
```

**Neo4j Adapter**:
```python
# adapters/neo4j.py

from neo4j import AsyncGraphDatabase
from .graph_base import GraphAdapter

class Neo4jAdapter(GraphAdapter):
    """Neo4j graph database adapter using async driver."""

    @property
    def database_type(self) -> str:
        return "neo4j"

    async def connect(self) -> None:
        self.driver = AsyncGraphDatabase.driver(
            f"neo4j://{self.host}:{self.port or 7687}",
            auth=(self.username, self.password)
        )
        self.is_connected = True

    async def create_node(self, label: str, properties: Dict[str, Any]) -> Any:
        async with self.driver.session() as session:
            result = await session.run(
                f"CREATE (n:{label} $props) RETURN id(n) as node_id",
                props=properties
            )
            record = await result.single()
            return record["node_id"]

    async def create_edge(self, from_id: Any, to_id: Any,
                         relationship: str, properties: Dict = None) -> Any:
        async with self.driver.session() as session:
            props = properties or {}
            result = await session.run(
                f"""
                MATCH (a), (b)
                WHERE id(a) = $from_id AND id(b) = $to_id
                CREATE (a)-[r:{relationship} $props]->(b)
                RETURN id(r) as edge_id
                """,
                from_id=from_id, to_id=to_id, props=props
            )
            record = await result.single()
            return record["edge_id"]

    async def traverse(self, start_id: Any, pattern: str, depth: int = 3) -> List[Dict]:
        async with self.driver.session() as session:
            # Example: pattern = "-[:FRIEND*1..3]->" for friends within 3 hops
            result = await session.run(
                f"""
                MATCH (start){pattern}(end)
                WHERE id(start) = $start_id
                RETURN end
                LIMIT 100
                """,
                start_id=start_id
            )
            records = await result.data()
            return records

    async def find_path(self, from_id: Any, to_id: Any,
                       relationship: str = None) -> List[List[Any]]:
        async with self.driver.session() as session:
            rel_pattern = f"[:{relationship}]" if relationship else ""
            result = await session.run(
                f"""
                MATCH path = shortestPath((a)-{rel_pattern}*-(b))
                WHERE id(a) = $from_id AND id(b) = $to_id
                RETURN path
                """,
                from_id=from_id, to_id=to_id
            )
            paths = await result.data()
            return paths
```

### 5.3 Graph-Specific Node Pattern

Instead of 9 CRUD nodes, generate **graph operation nodes**:

1. **CreateNodeNode** - Create graph node
2. **CreateEdgeNode** - Create relationship
3. **TraverseNode** - Graph traversal
4. **PathFindNode** - Shortest path
5. **SubgraphNode** - Extract subgraph
6. **UpdateNodeNode** - Update node properties
7. **UpdateEdgeNode** - Update edge properties
8. **DeleteNodeNode** - Delete node (and edges)
9. **DeleteEdgeNode** - Delete relationship

**Example**:
```python
# Auto-generated for @db.model Person in Neo4j
class PersonCreateNodeNode(AsyncNode):
    """Create a Person node in Neo4j."""

    async def execute_async(self, **inputs):
        node_id = await self.dataflow_instance.adapter.create_node(
            label="Person",
            properties=inputs
        )
        return {"node_id": node_id, **inputs}

class PersonCreateEdgeNode(AsyncNode):
    """Create relationship between Person nodes."""

    async def execute_async(self, **inputs):
        edge_id = await self.dataflow_instance.adapter.create_edge(
            from_id=inputs["from_id"],
            to_id=inputs["to_id"],
            relationship=inputs.get("relationship", "KNOWS"),
            properties=inputs.get("properties", {})
        )
        return {"edge_id": edge_id}
```

**Recommendation**: Graph databases require **separate adapter architecture** and **different node pattern**. Medium-to-high implementation effort.

---

## 6. Implementation Priority Ranking

### Prioritization Criteria

1. **Market Demand** (40%) - User requests, job postings, ecosystem size
2. **Technical Feasibility** (30%) - Async driver maturity, fit with current pattern
3. **Unique Value** (20%) - What it enables that others don't
4. **Implementation Effort** (10%) - LOE (inversely weighted - lower is better)

### Ranked Database Expansion (1-10)

| Rank | Database | Category | Demand | Feasibility | Value | Effort | Total Score | Phase |
|------|----------|----------|--------|-------------|-------|--------|-------------|-------|
| 1 | **pgvector** | Vector (PostgreSQL ext) | 9/10 | 10/10 | 10/10 | LOW | 9.5/10 | Phase 1 |
| 2 | **MongoDB** | Document | 10/10 | 9/10 | 9/10 | MEDIUM | 9.3/10 | Phase 1 |
| 3 | **TimescaleDB** | Time-series (PostgreSQL ext) | 8/10 | 10/10 | 9/10 | LOW | 8.9/10 | Phase 1 |
| 4 | **Qdrant** | Vector | 7/10 | 9/10 | 8/10 | MEDIUM | 8.0/10 | Phase 2 |
| 5 | **Redis** | Key-value | 9/10 | 8/10 | 6/10 | LOW | 7.6/10 | Phase 2 |
| 6 | **Neo4j** | Graph | 7/10 | 7/10 | 8/10 | HIGH | 7.3/10 | Phase 2 |
| 7 | **Milvus** | Vector | 6/10 | 7/10 | 8/10 | MEDIUM | 6.9/10 | Phase 3 |
| 8 | **InfluxDB** | Time-series | 6/10 | 7/10 | 7/10 | MEDIUM | 6.6/10 | Phase 3 |
| 9 | **DynamoDB** | Key-value (AWS) | 7/10 | 6/10 | 6/10 | MEDIUM | 6.4/10 | Phase 3 |
| 10 | **ArangoDB** | Multi-model | 5/10 | 6/10 | 7/10 | HIGH | 6.0/10 | Phase 3 |

---

## 7. Adapter Evolution Strategy

### 7.1 Current Architecture

```
DatabaseAdapter (base.py:16-128)
├── PostgreSQLAdapter (postgresql.py)
├── MySQLAdapter (mysql.py)
└── SQLiteAdapter (sqlite.py)
```

**Strengths**:
- Clean abstraction for SQL databases
- Shared connection pooling pattern
- Consistent transaction model
- Schema introspection

**Limitations**:
- SQL-centric (execute_query expects SQL strings)
- Table-centric schema model
- CRUD assumes relational structure

### 7.2 Proposed Evolution: Tiered Adapter Hierarchy

```
BaseAdapter (new - minimal interface)
├── DatabaseAdapter (current - SQL databases)
│   ├── PostgreSQLAdapter
│   ├── MySQLAdapter
│   ├── SQLiteAdapter
│   ├── PostgreSQLVectorAdapter (extends PostgreSQLAdapter + vector ops)
│   └── TimescaleDBAdapter (extends PostgreSQLAdapter + hypertables)
│
├── DocumentAdapter (new - document databases)
│   ├── MongoDBAdapter
│   └── CouchDBAdapter
│
├── VectorAdapter (new - specialized vector databases)
│   ├── MilvusAdapter
│   ├── QdrantAdapter
│   └── WeaviateAdapter
│
├── GraphAdapter (new - graph databases)
│   ├── Neo4jAdapter
│   ├── DgraphAdapter
│   └── ArangoDBAdapter (also implements DocumentAdapter)
│
└── KeyValueAdapter (new - key-value stores)
    ├── RedisAdapter
    └── DynamoDBAdapter
```

### 7.3 BaseAdapter Interface

```python
# adapters/base_minimal.py

from abc import ABC, abstractmethod
from typing import Any, Dict, List

class BaseAdapter(ABC):
    """Minimal base interface for all database adapters."""

    def __init__(self, connection_string: str, **kwargs):
        self.connection_string = connection_string
        self.is_connected = False
        self._config = kwargs

    @property
    @abstractmethod
    def database_type(self) -> str:
        """Return database type identifier (postgresql, mongodb, neo4j, etc.)."""
        pass

    @property
    @abstractmethod
    def adapter_category(self) -> str:
        """Return adapter category (sql, document, vector, graph, keyvalue)."""
        pass

    @abstractmethod
    async def connect(self) -> None:
        """Establish database connection."""
        pass

    @abstractmethod
    async def disconnect(self) -> None:
        """Close database connection."""
        pass

    @abstractmethod
    async def health_check(self) -> Dict[str, Any]:
        """Check database health and return status."""
        pass

    @property
    @abstractmethod
    def supports_transactions(self) -> bool:
        """Check if database supports transactions."""
        pass
```

### 7.4 Specialized Adapter Categories

**DocumentAdapter**:
```python
class DocumentAdapter(BaseAdapter):
    """Base class for document-oriented databases."""

    @property
    def adapter_category(self) -> str:
        return "document"

    @abstractmethod
    async def insert_document(self, collection: str, document: Dict) -> Any:
        pass

    @abstractmethod
    async def find_documents(self, collection: str, query: Dict, limit: int = 100) -> List[Dict]:
        pass

    @abstractmethod
    async def update_document(self, collection: str, query: Dict, update: Dict) -> Any:
        pass

    @abstractmethod
    async def delete_documents(self, collection: str, query: Dict) -> int:
        pass

    @abstractmethod
    async def aggregate(self, collection: str, pipeline: List[Dict]) -> List[Dict]:
        pass
```

**VectorAdapter**:
```python
class VectorAdapter(BaseAdapter):
    """Base class for vector databases."""

    @property
    def adapter_category(self) -> str:
        return "vector"

    @abstractmethod
    async def insert_vectors(self, collection: str, vectors: List[Dict]) -> Any:
        """Insert vectors with metadata."""
        pass

    @abstractmethod
    async def similarity_search(self, collection: str, query_vector: List[float],
                               k: int = 10, filter: Dict = None) -> List[Dict]:
        """Perform similarity search."""
        pass

    @abstractmethod
    async def create_index(self, collection: str, vector_field: str,
                          index_type: str, distance_metric: str) -> None:
        """Create vector index."""
        pass
```

**GraphAdapter**: (see section 5.2)

### 7.5 Hybrid Adapters

Some databases support multiple paradigms:

**ArangoDB** (document + graph):
```python
class ArangoDBAdapter(DocumentAdapter, GraphAdapter):
    """ArangoDB supports both document and graph operations."""

    @property
    def database_type(self) -> str:
        return "arangodb"

    @property
    def adapter_category(self) -> str:
        return "multimodel"  # Special category

    # Implement both DocumentAdapter and GraphAdapter methods
```

**PostgreSQL with pgvector** (relational + vector):
```python
class PostgreSQLVectorAdapter(PostgreSQLAdapter, VectorAdapter):
    """PostgreSQL with pgvector extension."""

    @property
    def adapter_category(self) -> str:
        return "sql+vector"

    # Inherits all PostgreSQL methods, adds vector operations
```

### 7.6 Migration Path

**Phase 1**: Keep current architecture, add new categories
- No breaking changes to existing PostgreSQL/MySQL/SQLite adapters
- Introduce `BaseAdapter` as new root
- Make `DatabaseAdapter` inherit from `BaseAdapter`
- Existing code continues to work

**Phase 2**: Implement specialized adapters
- `DocumentAdapter` for MongoDB
- `VectorAdapter` for pgvector (extends PostgreSQLAdapter)
- Add category detection to `DataFlow.__init__`

**Phase 3**: Full hierarchy
- `GraphAdapter`, `KeyValueAdapter`
- Hybrid adapters (ArangoDB, etc.)
- Backward compatibility maintained

---

## 8. Risk Analysis and Mitigation

### 8.1 Technical Risks

| Risk | Probability | Impact | Mitigation Strategy |
|------|------------|--------|-------------------|
| **Async driver immaturity** | MEDIUM | HIGH | Prioritize databases with proven async drivers (motor, asyncpg+pgvector) |
| **Schema mismatch** | HIGH | MEDIUM | Adapter-specific schema models, clear documentation |
| **Performance degradation** | LOW | HIGH | Benchmarking before/after, connection pool tuning |
| **Breaking changes to existing adapters** | LOW | HIGH | Strict backward compatibility testing, semantic versioning |
| **Transaction model mismatch** | MEDIUM | MEDIUM | Per-adapter transaction abstraction, clear feature flags |

### 8.2 Operational Risks

| Risk | Probability | Impact | Mitigation Strategy |
|------|------------|--------|-------------------|
| **Increased maintenance burden** | HIGH | MEDIUM | Tiered support (full vs. community), automated testing |
| **Documentation sprawl** | MEDIUM | MEDIUM | Template-based docs, auto-generation from adapters |
| **User confusion** | MEDIUM | HIGH | Clear decision guides, framework advisor tool |
| **Support requests** | HIGH | LOW | Comprehensive examples, troubleshooting guides |

### 8.3 Market Risks

| Risk | Probability | Impact | Mitigation Strategy |
|------|------------|--------|-------------------|
| **Low adoption of new adapters** | MEDIUM | LOW | User surveys before implementation, beta testing |
| **Database vendor changes** | LOW | MEDIUM | Abstract vendor-specific features, multiple options per category |
| **Competing frameworks** | MEDIUM | MEDIUM | Focus on unique value (zero-config + Kailash integration) |

---

## 9. Implementation Phases

### Phase 1: High-Value Extensions (Weeks 1-8)

**Goal**: Maximum value with minimal risk - extend existing PostgreSQL adapter

#### Week 1-2: pgvector Support
- Extend `PostgreSQLAdapter` with vector operations
- Add `VectorSearchNode`, `CreateVectorIndexNode`
- Integration tests with real pgvector extension
- Documentation: "Vector Search with PostgreSQL"

**Deliverables**:
- `PostgreSQLVectorAdapter` class
- 3 new node types (VectorSearch, VectorIndex, VectorUpsert)
- 20+ tests (unit + integration)
- User guide with RAG example

#### Week 3-4: TimescaleDB Support
- Extend `PostgreSQLAdapter` with hypertable operations
- Add `CreateHypertableNode`, `TimeRangeQueryNode`, `RetentionPolicyNode`
- Integration tests with real TimescaleDB
- Documentation: "Time-Series Data with TimescaleDB"

**Deliverables**:
- `TimescaleDBAdapter` class
- 4 new node types
- Time-series example (IoT sensor data)

#### Week 5-8: MongoDB Core
- Implement `MongoDBAdapter` from scratch
- Generate 9 standard nodes for MongoDB collections
- Add `AggregateNode`, `IndexNode`
- Integration tests with real MongoDB (replica set for transactions)
- Documentation: "Document Database with MongoDB"

**Deliverables**:
- Full `MongoDBAdapter` with motor integration
- 9 core nodes + 2 MongoDB-specific nodes
- 50+ tests across all tiers
- Migration guide from SQL to MongoDB

**Phase 1 Risk Level**: LOW - Leverages existing patterns, proven drivers

---

### Phase 2: Specialized Databases (Weeks 9-16)

**Goal**: Add specialized vector and graph databases

#### Week 9-11: Qdrant Vector Database
- Implement `QdrantAdapter` (async-first)
- Vector-specific node pattern
- Hybrid search examples
- Integration with Kaizen for embeddings

**Deliverables**:
- `QdrantAdapter` class
- Vector workflow examples
- Kaizen integration guide

#### Week 12-14: Neo4j Graph Database
- Implement `GraphAdapter` base class
- `Neo4jAdapter` with Cypher support
- Graph-specific node pattern (CreateNode, CreateEdge, Traverse, PathFind)
- Social network example

**Deliverables**:
- `GraphAdapter` base class
- `Neo4jAdapter` implementation
- Graph algorithm examples

#### Week 15-16: Redis Integration
- Implement `RedisAdapter` as caching layer
- NOT a primary database adapter
- Integration with DataFlow for query caching
- Pub/sub pattern for invalidation

**Deliverables**:
- `RedisAdapter` for caching
- Cache integration middleware
- Performance benchmarks

**Phase 2 Risk Level**: MEDIUM - New adapter categories, different query models

---

### Phase 3: Advanced & Multi-Model (Weeks 17-24)

**Goal**: Complete coverage with advanced databases

#### Week 17-19: Milvus Vector Database
- Implement `MilvusAdapter` with async wrapper
- Advanced indexing (HNSW, IVF, ANNOY)
- Benchmark vs. pgvector

**Deliverables**:
- `MilvusAdapter` class
- Performance comparison guide

#### Week 20-21: InfluxDB Time-Series
- Implement `InfluxDBAdapter`
- Flux query language integration
- Continuous queries

**Deliverables**:
- `InfluxDBAdapter` class
- Time-series analytics examples

#### Week 22-23: DynamoDB
- Implement `DynamoDBAdapter` (AWS)
- Handle eventual consistency
- Single-table design patterns

**Deliverables**:
- `DynamoDBAdapter` class
- AWS deployment guide

#### Week 24: ArangoDB Multi-Model
- Implement `ArangoDBAdapter` (document + graph)
- Demonstrate hybrid queries

**Deliverables**:
- `ArangoDBAdapter` class
- Multi-model examples

**Phase 3 Risk Level**: HIGH - Complex databases, vendor lock-in (DynamoDB)

---

## 10. Success Metrics

### Adoption Metrics
- **Downloads per database adapter**: Track individual adapter usage
- **GitHub stars/forks**: Indicator of community interest
- **User surveys**: Satisfaction scores per adapter

### Technical Metrics
- **Test coverage**: Maintain >90% for all adapters
- **Performance benchmarks**: Query latency, throughput for each database
- **Bug reports**: Track by adapter type

### Documentation Metrics
- **Example completeness**: Each adapter has 5+ working examples
- **Time-to-first-query**: How long to get started (target: <15 minutes)

---

## 11. Appendix: File References

### Current Implementation
- **Base adapter**: `src/dataflow/adapters/base.py:16-128`
- **PostgreSQL**: `src/dataflow/adapters/postgresql.py:16-424`
- **MySQL**: `src/dataflow/adapters/mysql.py:22-525`
- **SQLite**: `src/dataflow/adapters/sqlite.py:82-854`
- **Node generation**: `src/dataflow/core/nodes.py:81-135`
- **Connection parsing**: `src/dataflow/adapters/connection_parser.py`

### Architecture Documentation
- **ADR-001**: `docs/adr/001-dataflow-architecture.md` - 100% Kailash SDK integration
- **Framework guide**: `CLAUDE.md` - DataFlow patterns and critical rules

### Testing Structure
- Unit tests: `tests/unit/`
- Integration tests: `tests/integration/` (real databases, NO MOCKING)
- E2E tests: `tests/e2e/`

---

## 12. Recommendations Summary

### Immediate Actions (Next 30 Days)

1. **Implement pgvector support** - Extends PostgreSQL adapter, HIGH VALUE, LOW EFFORT
2. **Implement TimescaleDB support** - Extends PostgreSQL adapter, MEDIUM VALUE, LOW EFFORT
3. **Begin MongoDB adapter** - Highest demand document DB, MEDIUM EFFORT

### Strategic Priorities (6 Months)

1. **Phase 1 completion**: pgvector, TimescaleDB, MongoDB
2. **Tiered adapter architecture**: Implement `BaseAdapter`, `DocumentAdapter`, `VectorAdapter`
3. **Kaizen integration**: Seamless embedding generation for vector databases

### Long-Term Vision (12 Months)

1. **10 database support**: Cover all major categories (SQL, document, vector, graph, time-series, key-value)
2. **Hybrid adapters**: Multi-model databases (ArangoDB, PostgreSQL+pgvector)
3. **Auto-adapter selection**: Framework advisor suggests best database for use case
4. **Performance optimization**: Database-specific query optimization nodes

---

## Conclusion

DataFlow's adapter pattern has proven highly successful for SQL databases. Expanding to document, vector, graph, and time-series databases requires:

1. **Tiered adapter hierarchy** - Specialized base classes for each category
2. **Phased rollout** - Start with high-value, low-risk extensions (pgvector, TimescaleDB)
3. **MongoDB priority** - Best fit for document databases with proven async driver
4. **Vector integration with Kaizen** - Seamless AI agent workflows
5. **Maintain backward compatibility** - Existing adapters unchanged

**Recommended First Steps**:
1. Implement `PostgreSQLVectorAdapter` (pgvector support)
2. Implement `TimescaleDBAdapter` (time-series support)
3. Implement `MongoDBAdapter` (document database support)
4. Design `BaseAdapter` hierarchy for future expansion

This strategy balances **market demand**, **technical feasibility**, and **implementation effort** to deliver maximum value to DataFlow users.
